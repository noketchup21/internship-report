[
{
	"uri": "//localhost:1313/3-translatedblog/blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerate Marketing Campaign Planning by 3x with Treasure Data AI Agents Powered by Amazon Bedrock Marketing teams face significant challenges when planning and executing campaigns across multiple channels. Traditional campaign development requires months of coordination between systems and teams for hypothesis creation, audience analysis, journey mapping, content development, activation, and measurement. This lengthy process causes brands to miss critical moments when customers are ready to engage.\nTreasure Data’s customer data platform (CDP) serves major brands globally, managing customer profiles that represent a significant portion of the internet-connected population. Working with Amazon Web Services, Inc. (AWS), the company leveraged Amazon Bedrock to create AI-powered solutions for marketing teams. Amazon Bedrock provides fully managed access to high-performing foundation models for building generative AI applications, allowing organizations to deploy AI agents that understand natural language instructions and interact with various systems autonomously.\nThis blog explores how Treasure Data’s AI-powered offerings, built on Amazon Bedrock, transform campaign creation from a months-long process into hours or days. These solutions enable marketing and CX teams to respond quickly to market opportunities and deliver personalized experiences at scale while maintaining the security and governance standards required by enterprise customers.\nBuilding AI Agents on a Foundation of Trusted Data The true power of AI lies in combining advanced foundation models with high-quality customer data. The integration between Treasure Data’s platform and Amazon Bedrock enables marketers to analyze customer data quickly, generate targeted audience segments, create detailed personas, and make data-driven decisions without requiring technical expertise. This combination reduces campaign creation time dramatically while improving targeting accuracy and campaign performance.\nCollaborative Development with AWS Treasure Data worked closely with AWS to identify key bottlenecks in traditional campaign planning and execution processes. Rather than simply adding a chat interface to existing tools, the collaboration focused on redesigning fundamental workflows to maximize AI effectiveness.\nThe partnership emphasized finding the right balance between human expertise and AI capabilities. Marketing professionals retain strategic oversight while AI agents handle time-intensive analytical tasks. This approach required building agents that could process complex data relationships and provide actionable insights grounded in actual customer behavior.\nThe collaboration resulted in a multi-agent framework built on Amazon Bedrock that addresses specific marketing challenges while maintaining the security and compliance standards required by enterprise customers.\nHow Amazon Bedrock Powers This Innovation Treasure Data selected Amazon Bedrock as the foundation for its AI agents because it enables rapid deployment without sacrificing control or security. Amazon Bedrock simplifies model selection, allowing teams to access advanced foundation models without requiring specialized data science expertise.\nThe fully managed platform enables quick deployment to production environments without building custom infrastructure from scratch. Customer data remains private and secure within the shared responsibility model between AWS and customers. AWS secures the underlying infrastructure while customers maintain control over their content and access controls.\nThe combination of Treasure Data’s customer data expertise and AI foundation models, provided by Amazon Bedrock, enables organizations to scale AI initiatives while maintaining security and governance standards.\nMeet Treasure Data’s Specialized AI Agents Treasure Data has developed several purpose-built AI agents, powered by Amazon Bedrock, to address specific marketing challenges. Each agent targets critical pain points in the campaign planning and execution process.\nThe Audience Agent enables marketers to discover and create high-value audience segments from behavioral signals quickly without needing SQL or advanced data skills. Data analysis and audience segmentation become faster and more accurate as the agent identifies patterns in customer behavior automatically. Figure 1 shows Audience Agent, which retrieves customer data based on queries. For example, when asked ‘I’d like to understand my most loyal customers,’ it identifies relevant attributes and presents the results.\nFigure 1: Audience Agent Console\nThe Deep Research \u0026amp; Analysis Agent compresses hypothesis-building processes from months to less than a week. Instead of spending extensive time on manual analysis and marketing, customer teams can generate high-quality hypotheses grounded in behavioral signals that inform strategy, testing, and execution decisions. Treasure Data’s Deep Insight Platform provides “Question Management” capability, which lets users pose questions for multiple analyses such as Churn Rate Trend and Email Performance Analysis as shown in Figure 2.\nFigure 2: Treasure Data Deep Insight Platform\nAvailable as part of Treasure Data’s CDP Trade-Up program, the Migration Agent accelerates transitions from existing customer data platforms by up to 60%. It extracts queries, segments, and transformation logic from current systems and automatically generates SQL, pipelines, and orchestration workflows. This agent helps organizations preserve their segments, workflows, and business logic when moving data, avoiding the need to start from scratch.\nThese agents use retrieval augmented generation (RAG), combining data processing capabilities, with the power of Amazon Bedrock inference, to provide accurate and data-grounded responses. This ensures AI suggestions reflect actual customer behavior rather than generic recommendations.\nIntroducing Treasure Data’s AI Agent Foundry While the pre-built agents address common marketing challenges, Treasure Data customers expressed the need to create customized agents tailored to their unique business requirements and industry-specific use cases. The AI Agent Foundry emerged as the solution to this demand.\nThe AI Agent Foundry serves as the foundation for building custom AI agents tailored to specific business needs. Marketing, customer experience, and data teams can create, refine, and deploy agents without deep technical knowledge. High-impact use cases could include journey orchestration, data health monitoring, and campaign optimization specific to their organization.\nThe Foundry includes built-in security features, permission controls, auditability, and access management that meet enterprise governance requirements. Organizations can experiment with AI capabilities and deploy agents while maintaining data security, privacy and regulatory compliance. This approach enables customers to build agents that address their specific market dynamics and business processes.\nPractical Applications Driving Results The specialized agents address several critical marketing use cases through their integration with Amazon Bedrock. Decision support helps marketers evaluate multiple factors simultaneously when determining campaign targeting, messaging, and channel selection. The AI provides recommendations based on comprehensive data analysis rather than intuition alone.\nMultiple team members can collaborate with AI agents simultaneously, democratizing access to customer insights across marketing organizations. This capability eliminates bottlenecks caused by limited technical expertise on marketing teams.\nThe agents continuously learn from customer interactions and campaign performance, enabling organizations to refine their approach and achieve better results through rapid iteration and optimization.\nReal-World Impact: Nobitel Case Study Nobitel Co., Ltd., a leader in health and sports services, operates Dr. Stretch, a specialized stretching chain with 240+ locations across Japan. The company faced challenges with its marketing operations, where manual campaign planning and data silos prevented non-technical teams from accessing customer insights and delivering timely personalized offers.\nTo address these challenges, Nobitel implemented Treasure Data’s AI Agent Foundry, built on AWS AI/ML services including Amazon Bedrock. This implementation transformed their marketing operations, enabling non-technical marketers to execute personalized campaigns without advanced data skills. The results included 3x faster campaign planning and 20% improvement in store efficiency. Learn more about Nobitel’s transformation in their case study.\nThe Future of AI-Powered Marketing AI agents represent the beginning of a transformation that will reshape marketing and customer experience operations. Future developments will see agents testing messaging variations, generating creative content, orchestrating multi-channel campaigns, and optimizing spending in real-time across devices and regions.\nMarketing and CX professionals will evolve from campaign executors to strategic orchestrators. The critical question becomes whether data infrastructure can support numerous autonomous campaigns running simultaneously with precision and control.\nThis future requires robust data foundations, sophisticated AI capabilities, and governance frameworks that ensure trust and compliance at scale. Organizations building this infrastructure today position themselves to capitalize on autonomous marketing and CX operations.\nTransforming Marketing Through AI and Data Treasure Data’s specialized AI agents and AI Agent Foundry, powered by Amazon Bedrock, represent a fundamental shift in how marketing, CX, and data teams drive value from customer data. By combining trusted data with advanced foundation models, teams can analyze data, create segments, generate personas, and make strategic decisions in hours rather than months.\nThis transformation democratizes access to customer insights and automates complex analytical tasks. Marketing teams can respond faster to market opportunities and achieve better results through rapid iteration. The solution demonstrates that effective marketing requires both intelligent agents and the robust data infrastructure that makes them truly powerful.\nSecurity and compliance remain a shared responsibility between AWS and customers. AWS provides a secure, compliant foundation through Amazon Bedrock, while customers maintain control over their data and access policies. This approach enables organizations to innovate with AI while meeting their governance requirements.\nConclusion Treasure Data’s AI Agent Foundry and pre-built AI agents, powered by Amazon Bedrock, transform marketing campaign creation from a months-long process to just hours or days. These AI solutions enable marketers to quickly analyze data, create segments, generate personas, and make data-driven decisions without deep technical expertise. By democratizing access to customer insights and automating complex analytical tasks, all powered by Amazon Bedrock’s foundation models, marketing teams can now respond faster to market opportunities and achieve better results through rapid iteration.\nTreasure Data – AWS Partner Spotlight Treasure Data, an AWS partner, Treasure Data is the Intelligent Customer Data Platform purpose-built for enterprise scale. Trusted by Yum! Brands, Stellantis, AXA, and over 80 Global 2000 companies, Treasure Data is where trust, performance, and AI-first architecture converge to drive revenue with hyper-personalized customer experiences, lower marketing costs, and reduce risk. Treasure Data provides both out-of-the-box agents and the AI Agent Foundry that enables data-driven teams or partners to utilize, create, and deploy AI Agents on the Treasure Data platform and across their workflows while leveraging their data within the trusted Treasure Data environment.\nRonak Shah Ronak Shah is a Principal Partner Solution Architect with the AWS Industry Vertical team based in the New York City area. He works with AWS Partners in the Retail and CPG verticals to co-innovate on AWS. He is interested in finding new trends in retail and building innovative solutions in the areas of digital commerce, supply chain, customer experience and marketing technology. Outside of work, he volunteers at scouting and local debate competitions.\rHiroshi Nakamura Hiroshi Nakamura is an accomplished technology leader with extensive experience in software engineering and system architecture. Currently serving as CTO and VP of Engineering at Treasure Data since October 2014, Hiroshi has been instrumental in designing and developing a cloud-based Data Management platform capable of handling immense data volumes. An active open-source developer since April 1999, contributions include significant enhancements to Ruby and JRuby. Hiroshi holds a Master's degree in Science and Engineering from Waseda University.\rPranjal Gururani Pranjal Gururani is a Solutions Architect at AWS based out of Seattle. Pranjal works with various customers to architect cloud solutions that address their business challenges. He enjoys hiking, kayaking, skydiving, and spending time with family during his spare time.\r"
},
{
	"uri": "//localhost:1313/3-translatedblog/blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Build resilient generative AI agents Generative AI agents in production environments demand resilience strategies that go beyond traditional software patterns. AI agents make autonomous decisions, consume substantial computational resources, and interact with external systems in unpredictable ways. These characteristics create failure modes that conventional resilience approaches might not address.\nThis post presents a framework for AI agent resilience risk analysis that applies to most AI developments and deployment architectures. We also explore practical strategies to help prevent, detect, and mitigate the most common resilience challenges when deploying and scaling AI agents.\nGenerative AI agent resilience risk dimensions To identify resilience risks, we break down the generative AI agent systems into seven dimensions:\nFoundation models – Foundation models (FMs) provide core reasoning and planning capabilities. Your deployment choice determines your resilience responsibilities and costs. The three deployment approaches are fully self-managed such as using Amazon Elastic Compute Cloud (Amazon EC2), server-based managed services such as using Amazon SageMaker AI, or serverless managed services such as Amazon Bedrock. Agent orchestration – This component controls how multiple AI agents and tools coordinate to achieve complex goals, containing logic for tool selection, human escalation triggers, and multi-step workflow management. Agent deployment infrastructure – The infrastructure encompasses the underlying hardware and system where agents run. The infrastructure options include using fully self-managed EC2 instances, managed services such as Amazon Elastic Container Services (Amazon ECS), and specialized managed services designed specifically for agent deployment, such as Amazon Bedrock AgentCore Runtime. Knowledge base – The knowledge base includes vector database storage, embedding models, and data pipelines that create vector embeddings, forming the foundation for Retrieval Augmented Generation (RAG) applications. Amazon Bedrock Knowledge Bases supports fully managed RAG workflows. Agent tools – This includes API tools, Model Context Protocol (MCP) servers, memory management, and prompt caching features that extend agent capabilities. Security and compliance – This component encompasses user and agent security controls as well as content compliance monitoring, supporting proper authentication, authorization, and content validation. Security includes inbound authentication that manages users’ access to agents, and outbound authentication and authorization that manages agents’ access to other resources. Outbound authorization is more complex because agents might require their own identity. Amazon Bedrock AgentCore Identity is the identity and credential management service designed specifically for AI agents, providing inbound and outbound authentication and authorization capabilities. To help prevent compliance violations, organizations should establish comprehensive responsible AI policies. Amazon Bedrock Guardrails provides configurable safeguards for responsible AI policy implementation. Evaluation and observability – These systems track metrics from basic infrastructure statistics to detailed AI-specific traces, including ongoing performance evaluation and detection of behavioral deviations. Agent evaluation and observability requires a combination of traditional system metrics and agent-specific signals, such as reasoning traces and tool invocation results. The following diagram illustrates these dimensions.\nThis configuration provides visibility into agent applications, enabling subsequent sessions to deliver targeted resilience analysis and mitigation recommendations.\nTop 5 resilience problems for agents and mitigation plans The Resilience Analysis Framework defines fundamental failure modes that production systems should avoid. In this post, we identify generative AI agents’ five primary failure modes and provide strategies that can help establish resilient properties.\nShared fate Shared fate occurs when a failure in one agent component cascades across system boundaries, affecting the entire agent. Fault isolation is the desired property. To achieve fault isolation, you must understand how agent components interact and identify their shared dependencies.\nThe relationship between FMs, knowledge bases, and agent orchestration requires clear isolation boundaries. For example, in RAG applications, knowledge bases might return irrelevant search results. Implementing guardrails with relevance checks can help prevent these query errors from cascading through the rest of the agent workflow.\nTools should align with fault isolation boundaries to contain impact in case of failure. When building custom tools, design each tool as its own containment domain. When using MCP servers or existing tools, make sure you use strict, versioned request/response schemas and validate them at the boundary. Add semantic validations such as date ranges, cross-field rules, and data freshness checks. Internal tools can also be deployed across different AWS Availability Zones for additional resilience.\nAt the orchestration dimension, implement circuit breakers that monitor failure rates and latency, activating when dependencies become unavailable. Set bounded retry limits with exponential backoff and jitter to control cost and contention. For connectivity resilience, implement robust JSON-RPC error mapping and per-call timeouts, and maintain healthy connection pools to tools, MCP servers, and downstream services. The orchestration dimension should also manage contract-compatible fallbacks—routing from a failed tool or MCP server to alternatives—while maintaining consistent schemas and providing degraded functionality.\nWhen isolation boundaries fail, you can implement graceful degradation that maintains core functionality while advanced features become unavailable. Conduct resilience testing with AI-specific failure injection, such as simulating model inference failures or knowledge base inconsistencies, to test your isolation boundaries before problems occur in production.\nInsufficient capacity Excessive load can overwhelm even well-provisioned systems, potentially leading to performance degradation or system failure. Sufficient capacity makes sure your systems have the resources needed to handle both expected traffic patterns and unexpected surges in demand.\nAI agent capacity planning involves demand forecasting, resource assessment, and quota analysis. The primary consideration when planning capacity is estimating Requests Per Minute (RPM) and Tokens Per Minute (TPM). However, estimating RPM and TPM presents unique challenges due to the stochastic nature of agents. AI agents typically use recursive processing, where the agent’s reasoning engine repeatedly calls the FMs until reaching final answers. This creates two major planning difficulties. First, the number of iterative calls is hard to predict because it’s based on task complexity and reasoning paths. Second, each call’s token length is also hard to predict because it includes the user prompt, system instructions, agent-generated reasoning steps, and conversation history. This compounding effect makes capacity planning for agents difficult.\nThrough heuristic analysis during development, teams can set a reasonable recursion limit to help prevent redundant loops and runaway resource consumption. Additionally, because agent outputs become inputs for subsequent recursions, managing maximum completion tokens helps control one component of the growing token consumption in recursive reasoning chains.\nThe following equations help translate agent configurations to these capacity estimates:\nRPM = Average agent level thread per minute * average FM invocation per minute in one thread = Average agent level thread per minute * (1 + 60/(max_completion_tokens/TPS))\nToken per second (TPS) is different for each model, and can be found in model release documentation and open source benchmark results, such as artificial analysis.\nTPM = RPM * Average input token length = RPM * (system prompt length + user prompt length + max_completion_tokens * (recursion_limit -1)/recursion_limit)\nThis calculation is assuming no prompt caching feature is implemented.\nUnlike external tools where resilience is managed by third-party providers, internally developed tools rely on proper configuration by the development team to scale based on demand. When resource needs spike unexpectedly, only the affected tools require scaling.\nFor example, AWS Lambda functions can be converted to MCP-compatible tools using Amazon Bedrock AgentCore Gateway. If popular tools cause Lambda functions to reach capacity limits, you can increase the account-level concurrent execution limit or implement provisioned concurrency to handle the increased load.\nFor scenarios involving multiple action groups executing simultaneously, Lambda functions’ reserved concurrency controls provide essential resource isolation by allocating dedicated capacity to each action group. This helps prevent a single tool from consuming all available resources during orchestrated invocations, facilitating resource availability for high-priority functions.\nWhen capacity limits are reached, you can use intelligent request queuing with priority-based allocation to make sure essential services continue operating. Implementing graceful degradation during high-load periods can be helpful. This maintains core functionality while temporarily reducing non-essential features.\nExcessive latency Excessive latency compromises user experience, reduces throughput, and undermines the practical value of AI agents in production. Agentic workload development requires balancing speed, cost, and accuracy. Accuracy is the cornerstone for AI agents to gain user trust. Achieving high accuracy requires allowing agents to perform multiple reasoning iterations, which inevitably creates latency challenges.\nManaging user expectations becomes critical—establishing service level objective (SLO) metrics before project initiation sets realistic targets for agent response times. Teams should define specific latency thresholds for different agent capabilities, such as subsecond responses for simple queries vs. longer windows for analytical tasks requiring multiple tool interactions or extensive reasoning chains. Clear communication of the expected response times helps prevent user frustration and allows for appropriate system design decisions.\nPrompt engineering offers the greatest opportunity for latency improvement by reducing unnecessary reasoning loops. Vague prompts take agents into extensive deliberation cycles, whereas clear instructions accelerate decision-making. Asking an agent to “approve if the use case is of strategic value” creates a complex reasoning chain. The agent must first define strategic value criteria, then evaluate which criteria apply, and finally determine significance thresholds. Conversely, clearly stating the criteria in the system prompt can largely reduce agent iterations. The following examples illustrate the difference between ambiguous and clear instructions.\nThe following is an example of an ambiguous agent instruction:\nYou are a generative AI use case approver.\nYour role is to evaluate GenAI agent build requests by carefully analyzing user-provided information and make approval decisions. Please follow the following instructions:\n\u0026lt;instructions\u0026gt;\nCarefully analyze the information provided by the user, and collect use case information, such as use case sponsor, significance of the use case, and potential values that it can bring.\nApprove the use case if it has a senior sponsor and is of strategic value.\n\u0026lt;/instructions\u0026gt;\nThe following is an example of a clear, well-defined agent instruction:\nYou are a generative AI use case approver.\nYour role is to evaluate Gen AI agent build requests by carefully analyzing user-provided information and make approval decisions based on specific criteria.\nPlease strictly follow the following instructions:\n\u0026lt;instructions\u0026gt;\nCarefully analyze the information provided by the user. Collect answers to the following questions:\n\u0026lt;question_1\u0026gt;Does the use case have a business sponsor that is VP level and above?\u0026lt;/question_1\u0026gt;\n\u0026lt;question_2\u0026gt;What value is this agent expected to deliver? The answer can be in the form of\nnumber of hours per month saved on certain tasks, or additional revenue values.\u0026lt;/question_2\u0026gt;\n\u0026lt;question_3\u0026gt;If the use case is external customer facing, please provide supporting information\non the demand.\u0026lt;/question_3\u0026gt;\nEvaluate the request against these approval criteria:\n\u0026lt;criteria_1\u0026gt;The use case has business sponsor at VP level and above. This is a hard criteria.\u0026lt;/criteria_1\u0026gt;\n\u0026lt;criteria_2\u0026gt;The use case can bring significant $ value, calculated by productivity gain or\nrevenue increase. This is a soft criteria.\u0026lt;/criteria_2\u0026gt;\n\u0026lt;criteria_3\u0026gt;Have strong proof that the use case/feature is demanded by customers. This is a\nsoft criteria.\u0026lt;/criteria_3\u0026gt;\nBased on the evaluation, make a decision to approve or deny the use case.\nApprove: If the hard criterion is met, and at least one of the soft criteria is met. Deny: The hard criterion is not met, or neither of the soft criteria is met. \u0026lt;/instructions\u0026gt;\nPrompt caching delivers substantial latency reductions by storing repeated prompt prefixes between requests. Amazon Bedrock prompt caching can reduce latency by up to 85% for supported models, particularly benefiting agents with long system prompts and contextual information that remains stable across sessions.\nAsynchronous processing for agents and tools reduces latency by enabling parallel execution. Multi-agent workflows achieve dramatic speedups when independent agents execute in parallel rather than waiting for sequential completion. For agents with tools, asynchronous processing enables continued reasoning and preparation of subsequent actions while tools execute in the background, optimizing workflow by overlapping cognitive processing with I/O operations.\nSecurity and compliance checks must minimize latency impact while maintaining protection across dimensions. Content moderation agents implement streaming compliance scanning that evaluates agent outputs during generation rather than waiting for complete responses, flagging potentially problematic content in real time while allowing safe content to flow through immediately.\nIncorrect agent response Correct output makes sure your AI agent performs reliably within its defined scope, delivering accurate and consistent responses that meet user expectations and business requirements. However, misconfiguration, software bugs, and model hallucinations can compromise output quality, leading to incorrect responses that undermine user trust.\nTo improve accuracy, use deterministic orchestration flows whenever possible. Letting agents rely on LLMs to improvise their way through tasks creates opportunities to deviation from your intended path. Instead, define explicit workflows that specify how agents should interact and sequence their operations. This structured approach reduces both inter-agent calling errors and tool-calling mistakes. Additionally, implementing input and output guardrails significantly enhances agent accuracy. Amazon Bedrock Guardrails can scan user input for compliance checks before model invocations, and provide output validation to detect hallucinations, harmful responses, sensitive information, and blocked topics.\nWhen response quality issues occur, you can deploy human-in-the-loop validation for high-stakes decisions where accuracy is essential, and implement automatic retry mechanisms with refined prompts when initial responses don’t meet quality standards.\nSingle point of failure Redundancy creates multiple paths to success by minimizing single points of failure that can cause system-wide impairments. Single points of failure undermine redundancy when multiple components depend on a single resource or service, creating vulnerabilities that bypass protective boundaries. Effective redundancy requires both redundant components and redundant pathways, making sure that if one component fails, alternative components can take over, and if one pathway becomes unavailable, traffic can flow through different routes.\nAgents require coordinated redundancy for their FMs. If the models are self-managed, you can implement multi-Region model deployment with automated failover. When using managed services, Amazon Bedrock offers cross-Region inference to provide built-in redundancy for supported models, automatically routing requests to alternative AWS Regions when primary endpoints experience issues.\nThe agent tools dimension must coordinate tool redundancy to facilitate graceful degradation when primary tools become unavailable. Rather than failing entirely, the system should automatically route to alternative tools that provide similar functionality, even if they’re less sophisticated. For example, when the internal chat assistant’s knowledge base fails, it can fall back to a search tool to deliver alternative output to users.\nMaintaining permission consistency across redundant environments is essential. This helps prevent security gaps during failover scenarios. Because overly permissive access controls pose significant security risks, it’s critical to validate that both end-user permissions and tool-level access rights are identical between primary and failover components. This consistency makes sure security boundaries are maintained regardless of which environment is actively serving requests, helping prevent privilege escalation or unauthorized access that could occur when systems switch between different permission models during operational transitions.\nOperational excellence: Integrating traditional and AI-specific practices Operational excellence in agentic AI integrates proven DevOps practices with AI-specific requirements for running agentic systems reliably in production. Continuous integration and continuous delivery (CI/CD) pipelines orchestrate the full agent lifecycle, and infrastructure as code (IaC) standardizes deployments across environments, reducing manual error and improving reproducibility.\nAgent observability requires a combination of traditional metrics and agent-specific signals such as reasoning traces and tool invocation results. Although traditional system metrics and logs can be obtained from Amazon CloudWatch, agent-level tracing requires additional software build. The recently announced Amazon Bedrock AgentCore Observability (preview) supports OpenTelemetry to integrate agent telemetry data with existing observability services, including CloudWatch, Datadog, LangSmith, and Langfuse. For more details the Amazon Bedrock AgentCore Observability features, see Launching Amazon CloudWatch generative AI observability (Preview).\nBeyond monitoring, testing and validation of agents also extend beyond conventional software practices. Automated test suites such as promptfoo help development teams configure tests to evaluate reasoning quality, task completion, and dialogue coherence. Pre-deployment checks confirm tool connectivity and knowledge access, and fault injection simulates tool outages, API failures, and data inconsistencies to surface reasoning flaws before they affect users.\nWhen issues arise, mitigation relies on playbooks covering both infrastructure-level and agent-specific issues. These playbooks support live sessions, enabling seamless handoffs to fallback agents or human operators without losing context.\nSummary In this post, we introduced a seven-dimension architecture model to map your AI agents and analyze where resilience risks emerge. We also identified five common failure modes related to AI agents, and their mitigation strategies.\nThese strategies illustrated how resilience principles apply to common agentic workloads, but they are not exhaustive. Each AI system has unique characteristics and dependencies. You must analyze your specific architecture across the seven risk dimensions to identify the resilience challenges within your own workloads, prioritizing areas based on user impact and business criticality rather than technical complexity.\nResilience represents an ongoing journey rather than a destination. As your AI agents evolve and handle new use cases, your resilience strategies must evolve accordingly. You can establish regular testing, monitoring, and improvement processes to make sure your AI systems remain resilient as they scale. For more information about generative AI agents and resilience on AWS, refer to the following resources:\nChaos Engineering Scenarios for GenAI workloads Designing generative AI workloads for resilience Introducing Amazon Bedrock AgentCore: Securely deploy and operate AI agents at any scale (preview) Implement effective data authorization mechanisms to secure your data used in generative AI applications: Part 1 and Part 2 Yiwen Zhang Yiwen Zhang is a Principal GenAI Solutions Architect at AWS. She develops both generative AI applications and enterprise integration strategies. With a PhD in computational statistics and a decade of experience developing and deploying AI/ML and data pipelines, she brings deep technical expertise to enterprise AI transformation. She focuses on making generative AI practical by establishing successful generative AI MVPs and addressing challenging questions such as ROI, AI agent performance tuning, cost management, security, and production resilience. Her goal is to help enterprises build generative AI agents into “outcome as a service” solutions.\rHechmi Khelifi Hechmi Khelifi is an Enterprise Solutions Architect at AWS, focusing on resilience and reliability. With 3+ years at AWS and a PhD from the University of Quebec, Hechmi leverages his extensive IT experience and strong academic background to help customers build robust and resilient solutions.\rJennifer Moran Jennifer Moran is an AWS Senior Resilience Specialist Solutions Architect. She brings a wealth of experience from her diverse technical background, encompassing various roles across the software industry. Her expertise focuses on helping customers design resilient solutions to improve their overall resilience posture.\r"
},
{
	"uri": "//localhost:1313/3-translatedblog/blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Seamlessly burst EDA jobs to AWS using Synopsys Cloud Hybrid solution This post was contributed by Varun Shah, Xingang Zhao (Synopsys), Dnyanesh Digraskar, Mayur Runwal (AWS)\nIn this post we describe how customers can burst their Synopsys workloads to AWS using Synopsys Cloud Hybrid Solution. This simplifies the workflow by eliminating the need to manually move data between on-premises storage and the cloud. Data required by a job running on AWS is incrementally cached in the background and made available to the process. Similarly, when needed, selective data that’s written by a job running in the cloud can be pushed to on-premises in real time – keeping the on-premises storage up to date.\nThe semiconductor industry is experiencing explosive growth, driven by artificial intelligence (AI), high performance computing (HPC), and the proliferation of smart systems. Engineers are facing unprecedented complexity and an increasingly rapid pace of innovation. As chips become more powerful and intricate, design cycles grow longer, costs rise, and the risk of missing market windows increases. In this environment, time to market has become a critical differentiator.\nCloud-based infrastructure offers elasticity and flexibility to scale chip design operations and meet customer needs. AWS offers easy access to cloud compute resources which are optimized for EDA workloads. Amazon Elastic Compute Cloud (Amazon EC2) provides access to the latest generation processors in virtually unlimited capacity, deployable across the globe. Latest generation of x86 and ARM-based instances on AWS help realize maximum performance and let customers squeeze the most out of their EDA software licenses.\nSynopsys is an AWS Partner delivering trusted and comprehensive silicon to systems design solutions for customers, from EDA to silicon IP and system verification and validation.\nIn this post we’ll demonstrate scale-testing a set of Synopsys EDA tools on AWS and describe performance, quality of results, and turnaround time.\nSynopsys Cloud Hybrid Solution: bursting EDA jobs to AWS AWS makes it easy to setup cloud compute infrastructure with its large selection of hardware and services for semiconductor and hi-tech flows. Companies can match their compute needs with those of their time-critical workflows. At a high-level, the steps involved in bursting EDA workloads on AWS involve:\nIdentifying the design blocks/jobs that need to be run on the cloud Transferring relevant data to the cloud Installing EDA application binaries and licenses on the cloud Running EDA jobs Transferring the results back to on-premises for analysis/debug/reporting Steps 1 \u0026amp; 2 above can be cumbersome as packaging the right set of files, source code, libraries, dependencies, scripts etc. can take up to several weeks of time. Synopsys Cloud Hybrid Solution provides a cloud-enabled mechanism to accomplish these tasks more easily.\nArchitecture and Components Figure 1: Architectural diagram of Synopsys Cloud Hybrid solution with AWS used for testing described in this blog\nFigure 1 shows the high-level architecture of the Synopsys Cloud Hybrid solution. The key architectural components include:\nOn-premises storage: These are the NFS mount points with design data that must be presented to the compute nodes on AWS. The hybrid solution works with any third-party or commercial ISV storage solutions.\nScheduler: The Hybrid solution works with any commercially available job schedulers like the ones in AWS Parallel Computing Service (PCS), AWS ParallelCluster, AWS Batch, or IBM LSF, UGE, Slurm, etc. The architecture diagram illustrates the use of IBM LSF multi-cluster which integrates with LSF resource connector for provisioning resources on AWS.\nCompute farms:\nOn-premises: This is the datacenter with a mix of several different types of compute servers. On AWS: The Amazon EC2 instances used for performing the EDA simulations. For this testing, we used 16xlarge instances with 3.5GHz CPU, with 8GB memory per vCPU. A broad range of EC2 instance types could be used, depending on the application and semiconductor design under question. On Cloud NFS: For EDA jobs which write intermediate data that isn’t required on-premises, an Amazon FSx for NetApp ONTAP filesystem was provisioned for use by compute resources on AWS.\nSynopsys Cloud Hybrid Solution: Users install this binary on their Amazon EC2 instance and configure the data-sync to handle data movement between on-prem and AWS.\nLicensing:\nLicenses for EDA applications: Existing EDA application licenses and license servers continue to serve licenses for jobs running on AWS. Any additional license needs can be served by Synopsys FlexEDA and Pay-Per-Use metered licensing. License for Hybrid solution: This is a separate application that needs to be purchased by the users from Synopsys. The licensing is served from Synopsys managed network license servers. Setting up the Synopsys Cloud Hybrid Solution The setup includes 3 main components:\nHybrid Cloud data-sync: This entails installing the Hybrid solution on a recommended instance type on AWS, configure networking to allow traffic across on-premises and AWS resources for licensing (of Hybrid Cloud solution), data storage and distributed processing, identifying NFS storage mounts on-prem that require mapping and starting the Hybrid solution to establish the data-sync mapping. Network Connectivity: Dedicated network connections like AWS DirectConnect provide higher bandwidth, lower latency which makes it ideal for handling workloads with large data transfers. Alternatively, IPsec Virtual Private Network (VPN) tunnel can be used with AWS but offers less speed and reliability compared to dedicated network connections. Scheduler setup: This is specific to every scheduler. For example, in the case of IBM LSF, this involves deploying LSF multi-cluster, defining participating clusters, configuring queues and verifying that clusters on AWS can be dynamically provisioned as needed. General information about the platform With , selective data that is written by a job running on cloud, can be written back to on-premises in real time, thereby keeping the on-premises storage up to date. Note that data write-back incurs data egress charges from AWS. Temporary output data from running jobs that’s not required on-prem or large persistent output data can be directed to an NFS on-cloud. If necessary, this data can be scripted to write-back after job completion.\nThe job scheduler should be provisioned by the customer. It then finds an appropriate compute resource, on-premises or on AWS, and the Hybrid Solution handles the data movement necessary. Synopsys Cloud Hybrid Solution has ability to configure the scheduler to automatically exhaust on-premises compute capacity first and then schedule jobs on-cloud compute nodes, or to run all jobs on cloud compute nodes depending on the application being run and size/frequency of data being shared between different workers of the application.\nScale-testing Synopsys Hybrid Solution on AWS EDA jobs can be data intensive both in terms of input and output. The latency of the network and the amount of data moving back and forth can impact the performance of EDA jobs. To understand the boundaries of the Synopsys Hybrid Solution, we used six of Synopsys’ flagship applications which are essential for every semiconductor chip design process. These are:\nPrimeLib – a library characterization and validation application Synopsys VCS – Simulator for verifying semiconductor designs PrimeSim SPICE – GPU-accelerated SPICE Simulator for Analog, RF, and - Mixed-signal Design Synopsys PrimeWave – application for simulation setup and analysis of analog, RF, mixed-signal design, custom-digital and memory designs PrimeTime – static timing analysis application with fast, memory-efficient scalar and multicore computing Synopsys DSO.ai – an AI-powered design space optimization application All these EDA applications have different profiles in terms of I/O patterns, amount of data generated (size and volume of files), and the amount and frequency of communication that happens between different workers of a job. Each of these applications were executed using the Synopsys Cloud Hybrid Solution with standard designs and testcases. We prioritized jobs that require high IOPs when selecting the testcases for the benchmarking exercise.\nWe monitored several metrics and compared them between tests performed on Synopsys Cloud Solution on AWS and comparable on-premises infrastructure:\nPerformance: Runtime performance is a critical metric as it directly translates to time to market. Quality of Results (QoR): QoR is another critical metric that cannot be compromised on. The applications analyze the results in terms of coverage, passing tests and regressions and the output their interpretation of the overall quality of results. For the applications tested, we expect to see same or similar QoR results between AWS and on-premises tests. Setup time: If performance and QoR are reasonably on-par, then what makes Synopsys Cloud Hybrid Solution worthwhile is the time that can be saved in moving relevant data to the cloud to use AWS compute infrastructure. While it’s hard to measure this on-premises and differs greatly from one EDA flow to another, we tried to estimate this. Results Performance Table 1: Performance of Synopsys applications run on Hybrid Solution with AWS vs on-premises\nWe ran this baseline of tests either on-premises or on AWS depending on resource availability on premises. Table 1 shows that the overall performance of all EDA flows tested using Synopsys Cloud Hybrid Solution was comparable to – or in some cases, better than – the baseline. In the case of Hybrid Setup, the runtime includes time taken to write final output data back to on-premises storage through the Hybrid Solution. This impacts the overall performance slightly when compared to a baseline run natively on cloud like in the case of PrimeLib and VCS. The data also shows that performance was better than a baseline run on-premises for PrimeTime and PrimeSim. This is due to the availability of faster CPUs on AWS.\nThrough understanding IO patterns (size, frequency) of different EDA applications, we identified some best practices for setting up these applications to best use the Hybrid Solution.\nQuality of Results (QoR) As expected, there was no degradation in quality of results, PPA or coverage achieved when using the Hybrid Solution. For EDA applications like VCS, PrimeSim, results matched exactly with the runs on-premises. For other EDA applications like DSO.ai, the reports, PPA and QoR is comparable.\nSetup Time: Synopsys Hybrid Cloud saves up to several weeks of setup time per project by automating the manual data movement and verification of design blocks. For example, from our own internal testing of an EDA application’s QA regressions, we know that ~4 weeks of manual effort was saved because Hybrid Solution eliminated the need to lift and shift EDA workloads.\nDepending on the workflows, Synopsys can help in best practice recommendations on a cloud-native mode or a burst mode with Synopsys Cloud Hybrid Solution. With the true burst mode, data is written to on-cloud NFS, and then selective data is synced back to on-premises after the job completion. In this way, data egress and performance impacts are minimized. Burst mode adds huge value by incrementally moving input data as needed to the cloud for running the jobs through Hybrid Solution.\nConclusion EDA workloads demand immense computational power and seamless data access. The sheer volume of design data, coupled with the iterative and highly interdependent nature of EDA tasks, means that constant data movement between on-premises storage and cloud compute can become a slow and inefficient process.\nSynopsys Cloud Hybrid Solution solves this problem by providing a seamless way to burst EDA jobs from on-premises to the cloud with real-time two-way data synchronization. The product was stress-tested with six of Synospys’ flagship EDA applications. We found that performance of applications in Hybrid mode was comparable to performance of an on-premises or cloud-native run all while maintaining the same Quality of Results. Data movement was invisibly handled by Synopsys Cloud Hybrid Solution which saved up to weeks of setup time.\nVarun Shah Varun Shah is on the product management team for Synopsys Cloud, responsible for technology roadmap, cloud enablement of EDA software and voice-of-customer analysis. Varun holds a master's degree in electrical engineering from the University of Missouri-Rolla and is a certified product management professional from the Kellogg School of Management.\rDnyanesh Digraskar Dnyanesh Digraskar is a Principal HPC Partner Solutions Architect at AWS. He leads the HPC implementation strategy with AWS ISV partners to help them build scalable well-architected solutions. He has more than fifteen years of expertise in the areas of CFD, CAE, Numerical Simulations, and HPC. Dnyanesh holds a Master’s degree in Mechanical Engineering from University of Massachusetts, Amherst.\rMayur Runwal Mayur Runwal is a Senior Solutions Architect at AWS specializing in Electronic Design Automation (EDA). He architects semiconductor design and verification workflows for AWS customers. Before joining AWS, he led IT infrastructure teams at semiconductor companies for 10 years, where he designed and implemented virtual desktop solutions. His expertise includes high-performance computing, cloud architecture, and enterprise IT solutions.\rXingang Zhao Xingang Zhao is a Technical Product Manager for Synopsys Cloud. He's responsible for the technical assessment and integration of emerging cloud services and tools on public cloud platforms to boost the performance and cost-effectiveness of EDA workloads. Xingang holds a BS in electrical engineering from Zhejiang University and is a certified product management professional from the Kellogg School of Management.\r"
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.1-createvpc/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": "Create VPC Lab VPC Go to VPC service management console Click Your VPC. Click Create VPC. At the Create VPC page. In the Name tag field, enter Lab VPC. In the IPv4 CIDR field, enter: 10.10.0.0/16. Click Create VPC. "
},
{
	"uri": "//localhost:1313/4-events/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS AI/ML Services \u0026amp; Generative AI Workshop” Event Objectives Provide an overview of the AI/ML landscape in Vietnam. Demonstrate end-to-end machine learning workflows using Amazon SageMaker. Introduce Generative AI capabilities with Amazon Bedrock (Foundation Models, Agents, Guardrails). Share techniques for Prompt Engineering and Retrieval-Augmented Generation (RAG). Speakers AWS Solutions Architects AI/ML Specialists Key Highlights AWS AI/ML Services Overview (SageMaker) End-to-End Platform: Covered the full lifecycle from data preparation to deployment. Data Preparation: Strategies for labeling and cleaning data. Training \u0026amp; Tuning: optimizing models for performance and cost. Deployment: Moving models to production endpoints efficiently. Integrated MLOps: Highlighted capabilities to automate and standardize ML pipelines. SageMaker Studio: Live demo of the unified interface for building, training, and deploying models. Generative AI with Amazon Bedrock Foundation Models (FMs): Comparison and selection guide for top models: Claude: High reasoning capabilities. Llama: Open and efficient. Titan: Native AWS integration. Prompt Engineering: Chain-of-Thought: Breaking down complex reasoning tasks. Few-shot learning: Using examples to improve model output. Advanced Architectures: RAG (Retrieval-Augmented Generation): Integrating Knowledge Bases to ground answers in company data. Bedrock Agents: Creating multi-step workflows and integrating with external tools. Guardrails: Ensuring safety and content filtering. Key Takeaways Design Mindset Model Selection: Choose the right Foundation Model (e.g., Claude vs. Titan) based on the specific use case requirements (speed vs. reasoning). Safety First: Implementation of Guardrails is critical for responsible AI deployment in enterprise settings. Technical Architecture RAG over Fine-tuning: For most internal knowledge use cases, RAG provides a more flexible and cost-effective solution than fine-tuning models. Agentic Workflows: Moving from passive chatbots to active Agents that can execute tasks is the next frontier of GenAI. Modernization Strategy MLOps Adoption: Moving away from manual model training to automated pipelines (MLOps) is essential for scalability. Local Context: Understanding the specific AI/ML trends and landscape within Vietnam helps in benchmarking local projects. Applying to Work Pilot SageMaker: Evaluate current ML workflows and identify opportunities to migrate to Amazon SageMaker for better lifecycle management. Build a Knowledge Bot: Create a prototype using Amazon Bedrock and RAG to query internal documentation or technical manuals. Refine Prompting: Immediately apply Chain-of-Thought and Few-shot techniques to improve the accuracy of current AI interactions. Implement Guardrails: Configure content filters on Bedrock to ensure brand safety for any experimental applications. Event Experience Attending the “AWS AI/ML Services \u0026amp; Generative AI Workshop” provided a practical roadmap for adopting intelligent services. Key experiences included:\nLearning from experts Gained clarity on the AI/ML landscape in Vietnam, understanding local opportunities and challenges. Deepened technical knowledge on the differences between major Foundation Models (Claude, Llama, Titan). Hands-on technical exposure The SageMaker Studio walkthrough demonstrated how to unify the fragmented ML toolchain into a single pane of glass. The Live Demo of Bedrock showed the practical implementation of building a Generative AI chatbot, demystifying the complexity of RAG and Agents. Networking and discussions The ice-breaker activity and networking sessions allowed for exchanging ideas with peers about real-world challenges in deploying GenAI. Discussions reinforced the importance of Prompt Engineering as a critical skill for modern development. Lessons learned RAG is the key to making LLMs useful for business-specific data without the high cost of training. Guardrails are not optional; they are a fundamental layer of the Generative AI stack. Efficiency in ML comes from integrated MLOps rather than isolated data science experiments. Event photo "
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/",
	"title": "Preparing VPC and EC2",
	"tags": [],
	"description": "",
	"content": "In this step, we will need to create a VPC with 2 public / private subnets. Then create 1 EC2 Instance Linux located in the public subnet, 1 EC2 Instance Windows located in the private subnet.\nThe architecture overview after you complete this step will be as follows:\nTo learn how to create EC2 instances and VPCs with public/private subnets, you can refer to the lab:\nAbout Amazon EC2 Works with Amazon VPC Content Create VPC Create Public Subnet Create Private Subnet Create security group Create public Linux server Create private Windows server "
},
{
	"uri": "//localhost:1313/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "\rINTERNSHIP REPORT Student Information Full Name: Lương Nguyễn Duy Khang\nPhone Number: 0931984914\nEmail: lndkhang278@gmail.com\nUniversity: FPT University\nMajor: Software Engineer\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback Content Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/1-introduce/week1/",
	"title": "WEEK 1",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 1 objectives: Connected and formed groups with students at FCJ Proposed ideas about final project Studied and did labs on FCJ websites Understood basic concepts of cloud and AWS services Task carried out this week Day Task Start Date Completion Date Reference Material 2 - Get used to and take notes of First Cloud Journey tasks for students, its rules and learning materials 09/08/2025 09/08/2025 3 - Self study about cloud - Explore AWS services 09/09/2025 09/09/2025 Link 4 - Create AWS Free Tier account - Learn how to manage cost with AWS Budgets Do: + Create AWS account + Create different types of budgets 09/10/2025 09/10/2025 Link 5 - Study about EC2 - SSH connection methods to EC2 - Learn about Elastic IP - Propose ideas about final project - Study about VPC 09/11/2025 09/11/2025 Link 6 Do: + Create security groups + Create subnets + Launch an EC2 instance + Connect via SSH + Attach an EBS volume - Join a meeting and choose one idea for final project 09/12/2025 09/12/2025 Link Week 1 Achievements Understood what AWS is and learned the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Explored AWS services through self-study and documentation to understand their usage.\nLearned how to manage costs effectively with AWS Budgets, including creating different types of budgets.\nGained hands-on experience with EC2 by studying its features, connection methods (SSH), and Elastic IP.\nLearned the basics of VPC and networking setup in AWS.\nPracticed creating security groups, subnets, and launching an EC2 instance.\nSuccessfully connected to an EC2 instance via SSH and attached an EBS volume.\nWorked collaboratively to propose and select one idea for the final project.\n"
},
{
	"uri": "//localhost:1313/1-introduce/week2/",
	"title": "WEEK 2",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 2 Objectives Learn and practice AWS storage and database services (Amazon S3, Amazon RDS). Get familiar with and self-study Spring Boot to prepare for backend development. Gain knowledge of system monitoring with Amazon CloudWatch. Learn and practice deployment with Amazon Lightsail and Lightsail Container. Continue self-studying PostgreSQL to integrate with Spring Boot. Explore resource scaling with Amazon EC2 Auto Scaling. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Study Amazon S3 and Amazon RDS\n- Do: Amazon S3 labs, Amazon RDS labs\n- Self-study Spring Boot 09/15/2025 09/15/2025 AWS Study Group 3 - Self-study Spring Boot\n- Study Amazon CloudWatch 09/16/2025 09/16/2025 AWS Study Group 4 - Self-study Spring Boot and PostgreSQL\n- Do: Amazon CloudWatch labs 09/17/2025 09/17/2025 AWS Study Group 5 - Self-study Spring Boot and PostgreSQL\n- Study Amazon Lightsail and Lightsail Container\n- Do: Lightsail labs, Lightsail Container labs 09/18/2025 09/18/2025 AWS Study Group 6 - Self-study Spring Boot and PostgreSQL\n- Study Amazon EC2 Auto Scaling 09/19/2025 09/19/2025 AWS Study Group Week 2 Achievements Completed Amazon S3 labs and learned bucket creation, upload/download, and access management. Completed Amazon RDS labs, created databases, connected, and managed data. Self-studied and understood the basics of Spring Boot (project structure, dependencies, basic API creation). Learned about Amazon CloudWatch and practiced labs for monitoring, metrics, and alarms. Learned PostgreSQL and understood basic integration with Spring Boot. Completed Amazon Lightsail and Lightsail Container labs, deployed sample apps, and managed containers. Understood Amazon EC2 Auto Scaling mechanism and how instances scale automatically based on demand. "
},
{
	"uri": "//localhost:1313/1-introduce/week3/",
	"title": "WEEK 3",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 3 Objectives Set up the base project for backend development. Learn and practice Amazon Route 53. Continue learning about Amazon EC2 Auto Scaling. Develop core authentication features: register, login, and JWT services. Study and practice with Amazon CLI. Collaborate with the team to review project progress. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Set up base project for development\n- Study about Amazon Route 53\n- Do: Amazon EC2 Auto Scaling labs 09/22/2025 09/22/2025 AWS Study Group 3 - Start developing register feature\n- Do: Amazon Route 53 labs 09/23/2025 09/23/2025 AWS Study Group 4 - Continue developing register feature\n- Study about Amazon CLI 09/24/2025 09/24/2025 AWS Study Group 5 - Team meeting about project progress\n- Complete register and start developing login feature and JWT services 09/25/2025 09/25/2025 AWS Study Group 6 - Finish login and JWT services\n- Do: Amazon CLI labs 09/26/2025 09/26/2025 AWS Study Group Week 3 Achievements Successfully set up the base project structure for development. Completed labs on Amazon EC2 Auto Scaling and Amazon Route 53, gaining knowledge in scaling and DNS management. Implemented user registration feature and later expanded with login and JWT-based authentication. Practiced using Amazon CLI and completed related labs. Held a team meeting to discuss project progress and align development tasks. Completed core authentication flow (register + login + JWT services), providing a foundation for secure backend development. "
},
{
	"uri": "//localhost:1313/1-introduce/week4/",
	"title": "WEEK 4",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 4 Objectives Fix and optimize existing authentication features (register/login). Learn and integrate MoMo payment gateway into the project. Participate in team meetings to review progress and discuss project proposal. Study and practice with Amazon DynamoDB. Learn how to use Cloudinary for image uploads in Spring Boot. Complete labs on Amazon DynamoDB and integrate Cloudinary into the project. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Fix bugs in register/login services\n- Study about MoMo services and how to integrate in Spring Boot 09/29/2025 09/29/2025 AWS Study Group 3 - Team meeting about project progress\n- Develop and integrate MoMo payment in project 09/30/2025 09/30/2025 4 - Team meeting to discuss project proposal and assign different tasks for members\n- Finish MoMo payment integration 10/01/2025 10/01/2025 5 - Study about DynamoDB\n- Learn about Cloudinary and how to implement it for image uploads in Spring Boot 10/02/2025 10/02/2025 AWS Study Group 6 - Do labs about DynamoDB\n- Integrate Cloudinary in the project 10/03/2025 10/03/2025 AWS Study Group Week 4 Achievements Fixed and improved register/login services for better stability. Successfully learned and integrated MoMo payment gateway into the backend project. Participated in team meetings to review progress and coordinate new development tasks. Gained hands-on experience with Amazon DynamoDB through labs and practice. Learned how to use Cloudinary for efficient image storage and integrated it into the project. Strengthened overall backend functionality with payment and media upload features, preparing the system for future expansion. "
},
{
	"uri": "//localhost:1313/1-introduce/week5/",
	"title": "WEEK 5",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 5 Objectives Learn and practice Amazon ElastiCache to improve application performance through caching. Study and implement Google OAuth 2 for multi-login authentication options. Research and understand Amazon CloudFront and its use for content delivery and website acceleration. Complete labs for ElastiCache and CloudFront to gain practical experience. Study Edge Computing concepts with Amazon CloudFront and Lambda@Edge. Research and integrate MoMo IPN (Instant Payment Notification) to verify payment status. Learn and practice integrating Amazon S3 for file storage, upload, and download management. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Study about Amazon ElastiCache\n- Learn about Google OAuth 2 to integrate multiple login options in the project 10/06/2025 10/06/2025 AWS Study Group 3 - Study and research about Amazon CloudFront\n- Develop and integrate OAuth 2 into the project 10/07/2025 10/07/2025 AWS Study Group 4 - Do labs about Amazon ElastiCache and Amazon CloudFront\n- Finish integrating OAuth 2 10/08/2025 10/08/2025 AWS Study Group 5 - Study about Edge Computing with Amazon CloudFront and Lambda@Edge\n- Research about MoMo IPN to verify paid status 10/09/2025 10/09/2025 AWS Study Group 6 - Integrate MoMo IPN into project\n- Learn about integrating Amazon S3 to store, upload, and download files 10/10/2025 10/10/2025 AWS Study Group Week 5 Achievements Gained understanding of Amazon ElastiCache and its benefits for caching and performance optimization. Successfully implemented Google OAuth 2 for multiple login options, enhancing authentication flexibility. Completed labs and research on Amazon CloudFront, learning about CDN setup and content distribution. Practiced Edge Computing using Lambda@Edge to handle requests closer to users for lower latency. Integrated MoMo IPN into the project to automatically verify and confirm successful transactions. Learned and applied Amazon S3 integration for secure file storage, upload, and download functionality. Improved project scalability, authentication, and reliability through practical use of multiple AWS services. "
},
{
	"uri": "//localhost:1313/1-introduce/week6/",
	"title": "WEEK 6",
	"tags": [],
	"description": "",
	"content": "WEEK 6 WORKLOG Week 6 Objectives Integrate Amazon S3 into the project to support image upload and download functionality. Complete AWS CloudFront and Lambda@Edge labs to strengthen content delivery knowledge. Study Windows Workloads on AWS and Directory Services using AWS Managed Microsoft AD. Research and evaluate third-party APIs for calculating shipping fees, particularly Giao Hang Tiet Kiem (GHTK). Begin integrating the GHTK API into the existing project. Reinforce understanding of Secure Architectures in preparation for the midterm exam. Explore AWS VM Import/Export for virtual machine migration concepts. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Integrating S3 to the project with upload and download functions for images - Do labs related to Amazon CloudFront and Lambda@Edge 10/13/2025 10/13/2025 AWS Study Group 3 - Study about Windows Workloads on AWS and Directory Services with AWS Managed Microsoft AD - Considering and evaluating services to use to calculate shipping fee in the project 10/14/2025 10/14/2025 AWS Study Group 4 - Finish labs related to Windows Workloads on AWS and Directory Services with AWS Managed Microsoft AD - Research and study about Giao Hang Tiet Kiem API to calculate shipping fee 10/15/2025 10/15/2025 AWS Study Group GHTK API Docs 5 - Start to integrate Giao Hang Tiet Kiem API into the project - Do labs about Building Highly Available Web Applications - Join meeting about topic of Reinventing DevSecOps 10/16/2025 10/16/2025 AWS Study Group 6 - Revise knowledge about Secure Architecture to prepare for midterm exam - Continue to integrate Giao Hang Tiet Kiem API - Read about topic VM Migration with AWS VM Import/Export 10/17/2025 10/17/2025 AWS Study Group Week 6 Achievements Successfully integrated Amazon S3 into the project, enabling image upload and download features. Completed practical labs on AWS CloudFront and Lambda@Edge, gaining insights into edge computing and CDN optimization. Studied and finished labs on Windows Workloads and AWS Managed Microsoft AD, understanding how to manage hybrid environments. Researched and tested the GHTK shipping fee API, beginning integration into the project using Spring Boot. Participated in a team meeting about “Reinventing DevSecOps,” discussing security integration in CI/CD pipelines. Reviewed Secure Architecture principles, including IAM, encryption, and WAF, for midterm preparation. Read and summarized key points about VM migration using AWS VM Import/Export tools. "
},
{
	"uri": "//localhost:1313/1-introduce/week7/",
	"title": "WEEK 7",
	"tags": [],
	"description": "",
	"content": "WEEK 7 WORKLOG Week 7 Objectives Strengthen understanding of AWS architecture topics, including Resilient and High-Performing Architectures, in preparation for the midterm exam. Complete VM Migration using AWS VM Import/Export labs. Continue integrating and debugging the GHTK API for shipping fee calculation. Learn and apply concepts of Database Migration using AWS DMS and SCT. Study AWS Cost Optimization and Operational Excellence principles. Explore AWS Cognito for authentication integration in the ongoing project. Finalize the application architecture diagram using AWS services. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Revise knowledge about Resilient Architecture and services like EC2, S3,… to prepare for midterm exam - Complete VM Migration with AWS VM Import/Export labs - Continue to integrate and fix bugs with GHTK API 10/20/2025 10/20/2025 AWS Study Group GHTK API Docs 3 - Fix minor bugs with shopping cart after paying - Revised knowledge about High-Performing Architectures and services like RDS, DynamoDB,… 10/21/2025 10/21/2025 AWS Study Group 4 - Finish integrating GHTK API to calculate shipping fee - Read Database Migration with AWS Database Migration Service (DMS) and Schema Conversion Tool (SCT) - Team meeting to discuss and fix application diagram using AWS services 10/22/2025 10/22/2025 AWS Study Group GHTK API Docs 5 - Revise knowledge about AWS Cost Optimization and related services - Study about AWS Cognito 10/23/2025 10/23/2025 AWS Study Group 6 - Learn about Operational Excellence in AWS - Continue to learn about AWS Cognito to integrate into the project - Finish up application diagram with AWS services 10/24/2025 10/24/2025 AWS Study Group Week 7 Achievements Successfully revised and reinforced knowledge of AWS EC2, S3, RDS, and DynamoDB for resilient and high-performing architectures. Completed hands-on VM Migration using AWS VM Import/Export. Fully integrated and tested the GHTK API for calculating shipping fees within the project. Resolved bugs related to the shopping cart after payment. Collaboratively refined and finalized the application diagram with AWS services during team discussions. Gained a solid understanding of AWS Cost Optimization and Operational Excellence. Studied AWS Cognito and began planning its integration for user authentication. "
},
{
	"uri": "//localhost:1313/1-introduce/week8/",
	"title": "WEEK 8",
	"tags": [],
	"description": "",
	"content": "WEEK 8 WORKLOG Week 8 Objectives Learn and integrate AWS Cognito into the ongoing project to implement secure user authentication. Review and strengthen understanding of AWS core services (EC2, S3, IAM, MFA, SCP, Encryption, Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager). Study Resilient Architectures concepts including Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore in preparation for the AWS mid-term exam. Complete sample practice exams to evaluate and improve AWS service knowledge. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Learning AWS Cognito to integrate into the project - Redo labs related to EC2 and S3 services to strengthen knowledge for mid-term exam 10/27/2025 10/27/2025 AWS Study Group 3 - Start to integrate AWS Cognito into the project - Read articles and information related to IAM, MFA, SCP, and Encryption to prepare for the mid-term exam 10/28/2025 10/28/2025 AWS Study Group KungFuTech AWS Course 4 - Research about Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager to understand Secure Architectures - Do sample exams related to AWS services 10/29/2025 10/29/2025 AWS Study Group KungFuTech AWS Course Practice Exam 5 - Read articles related to Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore to understand Resilient Architectures - Do sample exams related to AWS services to strengthen knowledge 10/30/2025 10/30/2025 AWS Study Group KungFuTech AWS Course Practice Exam 6 - Participate in AWS mid-term exam 10/31/2025 10/31/2025 Week 8 Achievements Successfully learned how to use AWS Cognito and began integrating it into the project for authentication management. Revisited and completed multiple labs on EC2 and S3, reinforcing hands-on practical skills. Conducted extensive research on AWS security and resilience best practices, improving understanding of secure and scalable architectures. Completed several AWS practice exams, identifying areas for improvement and building exam confidence. Actively participated in the AWS mid-term exam, applying knowledge to practical and test-based scenarios. "
},
{
	"uri": "//localhost:1313/1-introduce/week9/",
	"title": "WEEK 9",
	"tags": [],
	"description": "",
	"content": "WEEK 9 WORKLOG Week 9 Objectives Continue integrating AWS Cognito into the project for user authentication and data storage. Learn and practice Disaster Recovery (DR) strategies using AWS Elastic Disaster Recovery. Study methods to optimize system performance using AWS services. Learn and implement Serverless Automation using AWS Lambda. Fix existing Redis-related issues in the project to ensure stability and performance. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Continue to integrate AWS Cognito into the project - Read articles to learn about Disaster Recovery and how to do it with AWS Elastic Disaster Recovery 11/03/2025 11/03/2025 AWS Study Group 3 - Do labs about Disaster Recovery with AWS Elastic Disaster Recovery - Fix bugs related to Redis in the project 11/04/2025 11/04/2025 AWS Study Group 4 - Successfully integrated AWS Cognito into the project for sign-up/login and saved data to AWS user pools - Continued improving by saving basic user information to the local database 11/05/2025 11/05/2025 AWS Study Group 5 - Read articles about optimizing the system using AWS - Learned how to save basic user information when signing up/login with AWS Cognito to local database 11/06/2025 11/06/2025 AWS Study Group 6 - Read about Serverless Automation with AWS Lambda - Completed the lab of Serverless Automation with AWS Lambda - Continued studying AWS services used in the final project for integration 11/07/2025 11/07/2025 AWS Study Group Week 9 Achievements Successfully completed AWS Cognito integration, enabling user sign-up/login and saving user information to AWS user pools and the local database. Gained hands-on experience with AWS Elastic Disaster Recovery, learning how to protect and recover workloads efficiently. Resolved Redis-related bugs, improving system stability and performance. Studied and applied system optimization techniques using AWS best practices. Learned and completed Serverless Automation with AWS Lambda, enhancing automation and serverless workflow skills. Strengthened overall understanding of AWS services relevant to the final project and prepared for future integrations. "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " Week 1: Getting familiar with AWS and basic AWS services. Week 2: Learning core AWS services (S3, RDS, CloudWatch, Lightsail, EC2 Auto Scaling) and practicing backend setup with Spring Boot and PostgreSQL. Week 3: Setting up the backend project, implementing authentication (register, login, JWT), and practicing with Route 53, EC2 Auto Scaling, and AWS CLI. Week 4: Improving backend authentication and integrating MoMo payment and Cloudinary while practicing core AWS services. Week 5: Exploring advanced AWS services and integrating OAuth 2, MoMo IPN, and Amazon S3 into the project. Week 6: Focusing on integrating Amazon S3 and the GHTK API, completing AWS labs, studying secure architectures, and preparing for the midterm exam. Week 7: Deepening understanding of AWS architectures and services while exploring AWS Cognito for project enhancement. Week 8: Strengthened AWS knowledge by studying secure and resilient architectures, practicing EC2 and S3 labs, integrating AWS Cognito, and completing the AWS midterm exam. Week 9: Enhanced AWS project by finalizing Cognito integration, practicing Disaster Recovery with AWS Elastic Disaster Recovery, learning system optimization, and automating workflows using AWS Lambda. Week 10: Strengthened Cognito integration, retrieved user attributes, resolved token issues, and improved system monitoring through CloudWatch and Grafana labs. Week 11: Advanced user management by syncing user updates between Cognito and the local database, completed AWS resource-management labs, and prepared secure logout implementation. Week 12: Improved Cognito user management, added admin controls, implemented secure logout, and prepared for AWS deployment. "
},
{
	"uri": "//localhost:1313/1-introduce/week10/",
	"title": "WEEK 10",
	"tags": [],
	"description": "",
	"content": "WEEK 10 WORKLOG Week 10 Objectives Learn AWS Cognito Identity Provider and prepare integration for Google Login. Study how to retrieve Cognito user attributes into the local database after registration. Explore methods to extract authenticated user information from Cognito. Resolve issues related to AWS Cognito tokens. Map Cognito token expiration to backend-generated tokens. Complete advanced monitoring labs using CloudWatch and Grafana. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Learn about Identity Provider in AWS Cognito and plan Google Login integration 11/10/2025 11/10/2025 AWS Study Group 3 - Learn how to retrieve Cognito user info to local DB - Complete Advanced Monitoring with CloudWatch \u0026amp; Grafana 11/11/2025 11/11/2025 AWS Study Group 4 - Implement methods to retrieve user information after Cognito authentication 11/12/2025 11/12/2025 5 - Successfully retrieve user info (name, email) to local DB - Fix token-related issues in Cognito flow 11/13/2025 11/13/2025 6 - Map Cognito token expiration to backend token expiration - Complete CloudWatch Advanced Workshop 11/14/2025 11/14/2025 AWS Study Group Week 10 Achievements Learned Cognito Identity Providers and prepared Google Login integration. Retrieved basic Cognito user information (name, email) into the local database. Implemented multiple methods to fetch user data after authentication. Fixed errors related to AWS Cognito tokens. Successfully mapped Cognito token expiration with backend token logic. Completed CloudWatch Advanced and Grafana monitoring labs, improving system observability and monitoring skills. "
},
{
	"uri": "//localhost:1313/1-introduce/week11/",
	"title": "WEEK 11",
	"tags": [],
	"description": "",
	"content": "WEEK 11 WORKLOG Week 11 Objectives Attend AWS Cloud Mastery #2 event at the AWS Office. Write a reflection paper for AWS Cloud Mastery #1. Complete labs on Resource Organization with Tags \u0026amp; Resource Groups. Complete lab on IAM \u0026amp; Resource Tag–based Access Control. Complete Systems Manager lab for automation and configuration. Develop API functionality to update user attributes in both local DB and AWS Cognito. Collaborate with the front-end team on feature alignment. Research secure logout implementation for Cognito users. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery #2 event - Write reflection paper for Cloud Mastery #1 11/17/2025 11/17/2025 3 - Complete lab on Resource Organization with Tags \u0026amp; Resource Groups - Complete IAM Tag-based Access Control lab 11/18/2025 11/18/2025 AWS Study Group 4 - Complete Systems Manager lab - Work on updating user attributes in local DB \u0026amp; Cognito 11/19/2025 11/19/2025 AWS Study Group 5 - Continue API development for updating user attributes - Team meeting with front-end team 11/20/2025 11/20/2025 6 - Continue API development for user attribute updates - Research Cognito logout implementation 11/21/2025 11/21/2025 Week 11 Achievements Attended AWS Cloud Mastery #2 and learned best practices for cloud architecture. Completed reflection paper for Cloud Mastery #1. Finished labs on Resource Tags, Resource Groups, and IAM Tag-based Permissions. Completed AWS Systems Manager lab, strengthening automation and operations knowledge. Made major progress on API to update user attributes in both Cognito and the local DB. Coordinated with front-end team to ensure consistent user management workflow. Researched secure logout options for Cognito, preparing for proper session handling. "
},
{
	"uri": "//localhost:1313/1-introduce/week12/",
	"title": "WEEK 12",
	"tags": [],
	"description": "",
	"content": "WEEK 12 WORKLOG Week 12 Objectives Write a reflection paper for AWS Cloud Mastery #2. Reconfigure Cognito login with new sub field mapped to the database. Implement secure Cognito logout handler. Fix authentication logic related to username during account creation. Add admin functions to disable/enable users in both local DB and AWS Cognito. Research AWS deployment workflow. Reconfigure routing after Cognito login. Refactor logout and routing logic. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Write reflection paper for AWS Cloud Mastery #2 - Reconfigure Cognito login with new sub field mapped to database 11/24/2025 11/24/2025 https://luma.com/39t066sy 3 - Implement Cognito logout handler - Fix authentication logic related to username during account creation 11/25/2025 11/25/2025 4 - Continue implementing Cognito logout handler - Add admin functions to enable/disable users in both local DB and AWS Cognito 11/26/2025 11/26/2025 5 - Research AWS deployment process - Reconfigure routing after Cognito login 11/27/2025 11/27/2025 https://cloudjourney.awsstudygroup.com/ 6 - Refactor logout functionality with AWS Cognito - Fix routing logic for login flow 11/28/2025 11/28/2025 Week 12 Achievements Completed reflection paper for AWS Cloud Mastery #2. Updated login flow to correctly handle the new Cognito sub field mapping. Implemented and refined secure Cognito logout handler. Fixed authentication logic related to username creation. Added admin functionality to enable/disable users in both Cognito and the local DB. Researched AWS deployment steps to prepare the project for hosting. Improved routing behavior for both login and logout flows. "
},
{
	"uri": "//localhost:1313/2-proposal/2.2-createiamrole/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Create IAM Role In this step, we will proceed to create IAM Role. In this IAM Role, the policy AmazonSSMManagedInstanceCore will be assigned, this is the policy that allows the EC2 server to communicate with the Session Manager.\nGo to IAM service administration interface In the left navigation bar, click Roles. Click Create role. Click AWS service and click EC2. Click Next: Permissions. In the Search box, enter AmazonSSMManagedInstanceCore and press Enter to search for this policy. Click the policy AmazonSSMManagedInstanceCore. Click Next: Tags. Click Next: Review. Name the Role SSM-Role in Role Name Click Create Role . Next, we will make the connection to the EC2 servers we created with Session Manager.\n"
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.2-createpublicsubnet/",
	"title": "Create Public Subnet",
	"tags": [],
	"description": "",
	"content": "Create Public Subnet Click Subnets. Click Create subnet. At the Create subnet page. In the VPC ID section, click Lab VPC. In the Subnet name field, enter Lab Public Subnet. In the Availability Zone section, select the first Availability zone. In the field IPv4 CIRD block enter 10.10.1.0/24. Scroll to the bottom of the page, click Create subnet.\nClick Lab Public Subnet.\nClick Actions. Click Edit subnet settings. Click Enable auto-assign public IPv4 address. Click Save. Click Internet Gateways. Click Create internet gateway. At the Create internet gateway page. In the Name tag field, enter Lab IGW. Click Create internet gateway. After successful creation, click Actions. Click Attach to VPC. At the Attach to VPC page. In the Available VPCs section, select Lab VPC. Click Attach internet gateway. Check the successful attaching process as shown below. Next we will create a custom route table to assign to Lab Public Subnet. Click Route Tables. Click Create route table. At the Create route table page. In the Name field, enter Lab Publicrtb. In the VPC section, select Lab VPC. Click Create route table. After creating the route table successfully. Click Edit routes. At the Edit routes page. Click Add route. In the Destination field, enter 0.0.0.0/0 In the Target section, select Internet Gateway and then select Lab IGW. Click Save changes. Click the Subnet associations tab. Click Edit subnet associations to proceed with the associate custom route table we just created in Lab Public Subnet. At the Edit subnet associations page. Click on Lab Public Subnet. Click Save associations. Check that the route table information has been associated with Lab Public Subnet and the internet route information has been pointed to the Internet Gateway as shown below. "
},
{
	"uri": "//localhost:1313/4-events/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “DevOps on AWS” Event Objectives Understand the core principles of DevOps culture and key performance metrics (DORA, MTTR). Master the AWS CI/CD toolchain for automating build, test, and deployment. Learn Infrastructure as Code (IaC) concepts using CloudFormation and AWS CDK. Explore containerization strategies using ECR, ECS, EKS, and App Runner. Implement full-stack observability and monitoring with CloudWatch and X-Ray. Speakers AWS DevOps Specialists Senior Solutions Architects Key Highlights DevOps Culture \u0026amp; Mindset Recap: Integration with AI/ML concepts from previous sessions. Metrics Matter: Focus on Deployment Frequency, Lead Time for Changes, Mean Time to Restore (MTTR), and Change Failure Rate (DORA metrics). Cultural Shift: Moving from siloed teams to shared responsibility. AWS CI/CD Pipeline Source Control: Utilizing AWS CodeCommit and implementing Git strategies like GitFlow and Trunk-based development. Build \u0026amp; Test: Configuring AWS CodeBuild for automated testing and compilation. Deployment: Using AWS CodeDeploy to implement safe deployment strategies: Blue/Green: Reduces downtime and risk. Canary: Gradual rollout to a small subset of users. Rolling: Update instances incrementally. Orchestration: Tying it all together with AWS CodePipeline. Infrastructure as Code (IaC) AWS CloudFormation: Defining infrastructure using templates, stacks, and managing configuration drift. AWS CDK (Cloud Development Kit): Using familiar programming languages to define cloud resources as code constructs. Comparison: Choosing between declarative templates (CloudFormation) vs. imperative code (CDK) based on team skills. Container Services \u0026amp; Observability Container Management: Storing images in Amazon ECR with lifecycle policies. Orchestration: Choosing between Amazon ECS (simpler, AWS-native) and Amazon EKS (Kubernetes standard), or AWS App Runner for simplified PaaS-like deployment. Monitoring: Using Amazon CloudWatch for metrics/alarms and AWS X-Ray for distributed tracing to identify performance bottlenecks. Key Takeaways DevOps Strategy Automation First: Manual deployments are error-prone; everything from infrastructure to code deployment should be automated. Measurement: You cannot improve what you do not measure. Use DORA metrics to track velocity and stability. Shift Left: Integrate testing and security early in the CI/CD pipeline, not at the end. Technical Architecture Immutable Infrastructure: Treat servers as disposable resources; replace them rather than patching them in place. Containerization: Decouple applications from the underlying OS to ensure consistency across environments (Dev, Test, Prod). Observability: Moving beyond simple \u0026ldquo;up/down\u0026rdquo; monitoring to deep insights using distributed tracing. Applying to Work Implement CI/CD: Set up a CodePipeline for current projects to automate the build/deploy process for Spring Boot/React applications. Adopt IaC: Start defining AWS resources (databases, S3 buckets, Cognito User Pools) using AWS CDK instead of the console. Containerize: Dockerize existing microservices and push images to ECR. Enhance Monitoring: Add X-Ray instrumentation to backend services to visualize API latency and database query performance. Event Experience Attending the “DevOps on AWS” workshop provided a practical roadmap for automating the software delivery lifecycle. It bridged the gap between writing code and running it reliably in production. Key experiences included:\nLearning from experts Gained clarity on the \u0026ldquo;Alphabet Soup\u0026rdquo; of AWS tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and how they integrate. Understood the strategic value of DORA metrics in justifying DevOps investments to business stakeholders. Hands-on technical exposure CI/CD Walkthrough: The demo of a full pipeline showed exactly how code changes trigger builds and deployments automatically. IaC Implementation: Seeing AWS CDK in action was a highlight—writing infrastructure in Java/TypeScript is much more intuitive for developers than writing JSON/YAML templates. Deployment Strategies: Visualizing Blue/Green deployments demonstrated how to release updates with zero downtime. Networking and discussions Discussed the trade-offs between ECS and EKS with peers, realizing that for many projects, ECS or App Runner provides a faster path to production with less overhead. Exchanged ideas on how to handle database migrations within a CI/CD pipeline. Lessons learned Drift Detection in CloudFormation is critical for maintaining infrastructure integrity. Observability is not optional for microservices; without X-Ray, debugging distributed architectures is nearly impossible. A solid DevOps foundation significantly reduces Mean Time to Recovery (MTTR), allowing teams to innovate faster with less fear of breaking production. Event photo "
},
{
	"uri": "//localhost:1313/2-proposal/",
	"title": "Proposal ",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.3-createprivatesubnet/",
	"title": "Create Private Subnet",
	"tags": [],
	"description": "",
	"content": "Create Private Subnet Click Subnets. Click Create subnet. At the Create subnet page. In the VPC ID section, click Lab VPC. In the Subnet name field, enter Lab Private Subnet. In the Availability Zone section, select the first Availability zone. In the field IPv4 CIRD block enter 10.10.2.0/24. Scroll to the bottom of the page, click Create subnet. The next step is to create the necessary security groups for the lab.\n"
},
{
	"uri": "//localhost:1313/4-events/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS Well-Architected Security Pillar” Event Objectives Master the five domains of the Security Pillar within the AWS Well-Architected Framework. Understand core security principles: Zero Trust, Least Privilege, and Defense in Depth. Learn to architect secure identity management (IAM) and infrastructure protection. Implement continuous detection and automated incident response strategies. Explore data protection techniques using KMS and encryption best practices. Speakers AWS Security Solutions Architects Cloud Security Specialists Key Highlights Security Foundation \u0026amp; Identity (IAM) Core Principles: Adopting a \u0026ldquo;Zero Trust\u0026rdquo; mindset and \u0026ldquo;Defense in Depth\u0026rdquo; (layering security controls). Threat Landscape: Analysis of top cloud security threats specific to the Vietnamese market. Modern IAM: Moving away from long-term credentials (access keys) to temporary credentials using IAM Roles. Access Control: Implementing IAM Identity Center for SSO and using Service Control Policies (SCPs) to set permission boundaries in multi-account environments. Validation: Using IAM Access Analyzer to verify policies and permissions. Detection \u0026amp; Infrastructure Protection Continuous Monitoring: Centralizing visibility with CloudTrail (org-level), GuardDuty (threat detection), and Security Hub. Logging Strategy: Enabling logs at every layer—VPC Flow Logs for network traffic and ALB/S3 logs for access patterns. Network Security: Implementing VPC segmentation (public vs. private subnets) and layering Security Groups (stateful) with NACLs (stateless). Edge Protection: Using AWS WAF and AWS Shield to protect against DDoS and web exploits. Data Protection \u0026amp; Incident Response Encryption: Managing keys with AWS KMS (key rotation, policies) and ensuring encryption at-rest (EBS, RDS, S3) and in-transit. Secrets Management: replacing hardcoded credentials with AWS Secrets Manager and Parameter Store. IR Playbooks: Defining standard procedures for scenarios like compromised IAM keys, accidental S3 public exposure, and EC2 malware events. Automation: Using Lambda and Step Functions to auto-remediate threats (e.g., isolating a compromised instance). Key Takeaways Security Mindset Shift Left: Security must be integrated early in the design phase, not added as an afterthought. Identity is the New Perimeter: In a cloud-native world, strong identity management (IAM) is more critical than traditional network firewalls. Assume Breach: Design systems assuming an attacker is already inside; focus on limiting the blast radius. Technical Architecture Least Privilege: Grant users and services only the permissions they need to perform their tasks, and nothing more. Detection-as-Code: Define security rules and alerts using code to ensure consistency and version control. Automated Rotation: Automate the rotation of database credentials and API keys to minimize the impact of leaked credentials. Applying to Work Audit IAM: Review current project IAM roles to remove unused permissions and enforce MFA for all users. Secure Secrets: Migrate database credentials from application.properties files to AWS Secrets Manager in Spring Boot applications. Enable GuardDuty: Turn on GuardDuty in all accounts to immediately start detecting anomalous behavior. Draft IR Plan: Create a basic Incident Response playbook for the most likely scenarios (e.g., application exploits). Event Experience The “AWS Well-Architected Security Pillar” session provided a deep dive into securing cloud workloads, balancing theoretical frameworks with practical implementation in the Vietnamese context.\nLearning from experts Gained insight into the Shared Responsibility Model, clarifying exactly which security aspects AWS handles vs. what the customer must secure. Learned about common pitfalls in Vietnam, such as inadvertent public data exposure, and how to prevent them. Hands-on technical exposure IAM Policy Simulation: The mini-demo on validating policies showed how to simulate access requests to ensure permissions are configured correctly before deployment. IR Lifecycle: Walking through the \u0026ldquo;Compromised IAM Key\u0026rdquo; playbook highlighted the importance of speed and automation in responding to incidents. Networking and discussions Discussed the challenges of implementing Zero Trust in legacy application architectures. Exchanged tips on preparing for the AWS Certified Security – Specialty exam. Lessons learned Log Everything: You cannot detect what you do not log. VPC Flow Logs and CloudTrail are essential for forensics. Avoid Long-Term Keys: Static IAM access keys are a major risk; switching to IAM Roles for EC2/Lambda is a priority. Automation is Security: Manual security checks scale poorly; automation using EventBridge and Lambda allows for real-time remediation. Event photo "
},
{
	"uri": "//localhost:1313/4-events/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Agentic AI \u0026amp; Orchestration on AWS” Event Objectives Explore the high-level architecture of Agentic AI using Amazon Bedrock. Understand the transition from standard GenAI to \u0026ldquo;Agentic\u0026rdquo; workflows that can execute tasks. Learn advanced orchestration techniques and context optimization (L300 level). Examine real-world case studies of agent deployment from Diaflow and CloudThinker. Gain hands-on experience building agents through interactive coding sessions. Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Kien Nguyen – Solutions Architect, AWS Viet Pham – Founder \u0026amp; CEO, Diaflow Thang Ton – Co-founder \u0026amp; COO, CloudThinker Henry Bui – Head of Engineering, CloudThinker Kha Van – Community Leader Key Highlights AWS Bedrock Agent Core Foundations: Technical deep-dive into the architecture of Amazon Bedrock Agents. Capabilities: Moving beyond simple text generation to agents that can plan, reason, and execute actions via API calls. Architecture: Understanding how the agent router, action groups, and knowledge bases interact to solve complex user queries. Real-World Implementation (Diaflow) Case Study: Presentation by Diaflow’s CEO on building practical agentic workflows. Workflow Automation: Demonstrating how to chain multiple AI steps together to create a cohesive business process. Challenges: Discussing the hurdles of moving from prototype to production in a real-world startup environment. Advanced Orchestration (CloudThinker) L300 Technical Deep Dive: Focused on the engineering challenges of AI orchestration. Context Optimization: Techniques for managing the context window effectively on Amazon Bedrock to reduce costs and improve accuracy. Orchestration Framework: How CloudThinker structures the interaction between different models and tools to maintain state and intent. Hands-on Workshop CloudThinker Hack: An interactive coding session led by engineers. Implementation: Moving from theory to code, setting up the basic structure of an agent and connecting it to AWS services. Key Takeaways Design Mindset Agents vs. Chatbots: The shift from passive information retrieval (Chatbots) to active task execution (Agents). Context is King: Optimizing what goes into the context window is as important as the model itself; too much noise degrades performance. Orchestration: Successful agents require a robust layer to manage decision-making logic and state. Technical Architecture Tool Use: Agents are only as good as the tools (APIs/Functions) you give them access to. State Management: Handling multi-turn conversations requires careful state tracking to ensure the agent remembers previous constraints. Optimization: Advanced context strategies are necessary to handle long conversations without hitting token limits or latency issues. Applying to Work Experiment with Bedrock Agents: Start building a simple agent that connects to the \u0026ldquo;Meal Plan\u0026rdquo; project backend to answer user queries (e.g., \u0026ldquo;Find me a recipe under 500 calories\u0026rdquo;). Refine Context: Review how user data is passed to the LLM; implement context pruning to keep prompts efficient. Workflow Integration: Explore using agentic workflows to automate backend administrative tasks (e.g., approving user content or verifying images). Event Experience The “Agentic AI \u0026amp; Orchestration on AWS” event was a comprehensive journey from high-level concepts to L300 engineering details. It highlighted the rapid evolution of AI from \u0026ldquo;thinking\u0026rdquo; to \u0026ldquo;doing.\u0026rdquo;\nLearning from experts The L300 session by Henry Bui was particularly insightful regarding Context Optimization, offering concrete strategies to improve model performance and reduce latency. Gained a clear understanding of the Amazon Bedrock ecosystem and how it simplifies the complexity of building custom agents. Hands-on technical exposure CloudThinker Hack: The workshop provided a much-needed practical component, allowing us to see the code behind the concepts. Real-world examples: Seeing Diaflow\u0026rsquo;s implementation proved that Agentic AI is ready for production business use cases, not just research. Networking and discussions Connect with the CloudThinker team during the lunch buffet to discuss specific challenges in orchestrating multi-agent systems. Discussed with peers how to integrate these agents into existing Spring Boot and React architectures. Lessons learned Action Groups in Bedrock are the key to unlocking real value; defining clear API schemas for the agent is critical. Orchestration is complex; using a framework or a managed service like Bedrock Agents is preferred over building the routing logic from scratch. The future of app development is Agent-driven, where the UI adapts to the user\u0026rsquo;s intent rather than forcing the user to navigate static menus. Event photo "
},
{
	"uri": "//localhost:1313/3-translatedblog/",
	"title": "Translated blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1- Accelerate Marketing Campaign Planning by 3x with Treasure Data AI Agents Powered by Amazon Bedrock The article explains how Treasure Data, in partnership with Amazon Bedrock (AWS), uses AI Agents to accelerate marketing campaign planning by 3x. These agents—like Audience Agent, Research Agent, and Migration Agent—analyze customer data, segment audiences, and automate insights without technical expertise. With AI Agent Foundry, businesses can create custom agents to meet specific needs while maintaining security and compliance. A case study with Nobitel showed campaign planning time cut by threefold and a 20% performance boost. Overall, the solution empowers marketers to focus on strategy while AI handles analysis, driving faster, data-driven, and more personalized marketing.\nBlog 2- Build resilient generative AI agents The article by Yiwen Zhang, Hechmi Khelifi, and Jennifer Moran outlines how to build resilient generative AI agents for production. It presents a seven-part framework covering models, infrastructure, tools, and security, and identifies five failure modes like latency and inaccurate responses. The authors suggest solutions such as fault isolation, redundancy, and human-in-the-loop validation, combining DevOps with AI-specific safeguards like Amazon Bedrock Guardrails. Overall, it offers guidance to ensure reliable, scalable AI deployments.\nBlog 3- Seamlessly burst EDA jobs to AWS using Synopsys Cloud Hybrid solution The article highlights how Synopsys Cloud Hybrid, powered by AWS, lets semiconductor teams easily run EDA workloads on the cloud without manual data transfers. It syncs on-prem and cloud data in real time using Amazon EC2 and FSx for NetApp ONTAP, delivering equal or better performance than on-prem setups. Supporting tools like VCS and PrimeTime, it also cuts setup time by weeks, offering a scalable, fast, and cost-efficient solution for chip design on AWS.\n"
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.4-createsecgroup/",
	"title": "Create security groups",
	"tags": [],
	"description": "",
	"content": "Create security groups In this step, we will proceed to create the security groups used for our instances. As you can see, these security groups will not need to open traditional ports to ssh like port 22 or remote desktop through port 3389.\nCreate security group for Linux instance located in public subnet Go to VPC service management console Click Security Group. Click Create security group. In the Security group name field, enter SG Public Linux Instance. In the Description section, enter SG Public Linux Instance. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Keep Outbound rule, drag the mouse to the bottom. Click Create security group. As you can see, the security group we created to use for Linux public instances will not need to open traditional ports to ssh like port 22.\nCreate a security group for a Windows instance located in a private subnet After successfully creating a security group for the Linux instance located in the public subnet, click the Security Groups link to return to the Security groups list. Click Create security group.\nIn the Security group name field, enter SG Private Windows Instance.\nIn the Description section, enter SG Private Windows Instance. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Scroll down. Add Outbound rule to allow TCP 443 connection to 10.10.0.0/16 ( CIDR of Lab VPC we created) Click Create security group. For the Instance in the private subnet, we will connect to the Session Manager endpoint over a TLS encrypted connection, so we need to allow outbound connection from our instance to VPC CIDR through port 443.\nCreate security group for VPC Endpoint In this step, we will create security group for VPC Endpoint of Session Manager. After successfully creating the security group for the Windows instance in the private subnet, click the Security Groups link to return to the Security groups list. Click Create security group. In the Security group name field, enter SG VPC Endpoint. In the Description section, enter SG VPC Endpoint. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Scroll down. Delete Outbound rule. Add Inbound rule allowing TCP 443 to come from 10.10.0.0/16 ( CIDR of Lab VPC we created ). Click Create security group. So we are done creating the necessary security groups for EC2 instances and VPC Endpoints.\n"
},
{
	"uri": "//localhost:1313/4-events/",
	"title": "Events participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.5-createec2linux/",
	"title": "Create Public instance",
	"tags": [],
	"description": "",
	"content": " Go to EC2 service management console Click Instances. Click Launch instances. On the Step 1: Choose an Amazon Machine Image (AMI) page. Click Select to select AMI Amazon Linux 2 AMI. On the Step 2: Choose an Instance Type page. Click on Instance type t2.micro. Click Next: Configure Instance Details. At Step 3: Configure Instance Details page In the Network section, select Lab VPC. In the Subnet section, select Lab Public Subnet. In the Auto-assign Public IP section, select Use subnet setting (Enable) Click Next: Add Storage. Click Next: Add Tags to move to the next step. Click Next: Configure Security Group to move to the next step. On page Step 6: Configure Security Group. Select Select an existing security group. Select security group SG Public Linux Instance. Click Review and Launch. The warning dialog box appears because we do not configure the firewall to allow connections to port 22, Click Continue to continue.\nAt page Step 7: Review Instance Launch.\nClick Launch. In the Select an existing key pair or create a new key pair dialog box. Click to select Create a new key pair. In the Key pair name field, enter LabKeypair. Click Download Key Pair and save it to your computer. Click Launch Instances to create EC2 server. Click View Instances to return to the list of EC2 instances.\nClick the edit icon under the Name column.\nIn the Edit Name dialog box, enter Public Linux Instance. Click Save. Next, we will do the same to create an EC2 Instance Windows running in the Private subnet.\n"
},
{
	"uri": "//localhost:1313/5-portfwd/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nWe will configure Port Forwarding for the RDP connection between our machine and Private Windows Instance located in the private subnet we created for this exercise.\nCreate IAM user with permission to connect SSM Go to IAM service management console Click Users , then click Add users. At the Add user page. In the User name field, enter Portfwd. Click on Access key - Programmatic access. Click Next: Permissions. Click Attach existing policies directly.\nIn the search box, enter ssm. Click on AmazonSSMFullAccess. Click Next: Tags, click Next: Reviews. Click Create user. Save Access key ID and Secret access key information to perform AWS CLI configuration.\nInstall and Configure AWS CLI and Session Manager Plugin To perform this hands-on, make sure your workstation has AWS CLI and Session Manager Plugin installed -manager-working-with-install-plugin.html)\nMore hands-on tutorials on installing and configuring the AWS CLI can be found here.\nWith Windows, when extracting the Session Manager Plugin installation folder, run the install.bat file with Administrator permission to perform the installation.\nImplement Portforwarding Run the command below in Command Prompt on your machine to configure Port Forwarding. aws ssm start-session --target (your ID windows instance) --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region (your region) Windows Private Instance Instance ID information can be found when you view the EC2 Windows Private Instance server details.\nExample command: C:\\Windows\\system32\u0026gt;aws ssm start-session --target i-06343d7377486760c --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region ap-southeast-1 If your command gives an error like below: SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found\nProve that you have not successfully installed the Session Manager Plugin. You may need to relaunch Command Prompt after installing Session Manager Plugin.\nConnect to the Private Windows Instance you created using the Remote Desktop tool on your workstation. In the Computer section: enter localhost:9999. Return to the administration interface of the System Manager - Session Manager service. Click tab Session history. We will see session logs with Document name AWS-StartPortForwardingSession. Congratulations on completing the lab on how to use Session Manager to connect and store session logs in S3 bucket. Remember to perform resource cleanup to avoid unintended costs.\n"
},
{
	"uri": "//localhost:1313/2-proposal/2.1-createec2/2.1.6-createec2windows/",
	"title": "Create Private Instance",
	"tags": [],
	"description": "",
	"content": " Go to EC2 service management console Click Instances. Click Launch instances. On the Step 1: Choose an Amazon Machine Image (AMI) page. Drag the mouse down. Click Select to select AMI Microsoft Windows Server 2019 Base. On the Step 2: Choose an Instance Type page. Click on Instance type t2.micro. Click Next: Configure Instance Details. At Step 3: Configure Instance Details page In the Network section, select Lab VPC. In the Subnet section, select Lab Private Subnet. At Auto-assign Public IP select Use subnet setting (Disable) Click Next: Add Storage. Click Next: Add Tags to move to the next step. Click Next: Configure Security Group to move to the next step. On page Step 6: Configure Security Group. Select Select an existing security group. Select security group SG Private Windows Instance. Click Review and Launch. The warning dialog box appears because we do not configure the firewall to allow connections to port 22, Click Continue to continue.\nAt page Step 7: Review Instance Launch.\nClick Launch. In the Select an existing key pair or create a new key pair dialog box. Click Choose an existing key pair. In the Key pair name section, select LabKeypair. Click I acknowledge that I have access to the corresponding private key file, and that without this file, I won\u0026rsquo;t be able to log into my instance.. Click Launch Instances to create EC2 server. Click View Instances to return to the list of EC2 instances.\nClick the edit icon under the Name column.\nIn the Edit Name dialog box, enter Private Windows Instance. Click Save. Next, we will proceed to create IAM Roles to serve the Session Manager.\n"
},
{
	"uri": "//localhost:1313/6-self-assessment/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS First Cloud Journey from 08/09/2025 to *\t28/12/2025, I was able to apply what I had learned in school to real development tasks and gain practical industry experience. I worked on a meal-planning application, a system designed to support users who want to cook but lack time to shop for ingredients. Through this project, I expanded my technical abilities in Spring Framework, PostgreSQL, AWS, and strengthened my skills in problem-solving, teamwork, and software design.\nThroughout the internship, I maintained a responsible work attitude, followed company processes, and regularly communicated with teammates to ensure progress and quality.\nBelow is my self-evaluation based on several key aspects of my internship experience:\nNo. Criteria Description Good Fair Average 1 Technical competence Understanding core concepts, applying knowledge effectively, and using tools efficiently [ ] ✅ [ ] 2 Learning adaptability Ability to pick up new technologies and adapt to new tasks quickly [ ] ✅ [ ] 3 Initiative Readiness to take action, suggest tasks, and contribute without being prompted [ ] ✅ [ ] 4 Accountability Completing assigned work on schedule and ensuring reliable output ✅ [ ] [ ] 5 Workplace discipline Following company policies, schedules, and established workflows ✅ [ ] [ ] 6 Growth mindset Openness to feedback and willingness to improve skills and performance ✅ [ ] [ ] 7 Communication clarity Ability to present ideas clearly and provide concise, effective updates [ ] [ ] ✅ 8 Collaboration Ability to coordinate with team members and contribute positively in group settings [ ] [ ] ✅ 9 Professional behavior Respectfulness, appropriate conduct, and positive attitude within the work environment ✅ [ ] [ ] 10 Analytical \u0026amp; problem-solving Identifying issues, analyzing root causes, and proposing logical and creative solutions [ ] ✅ [ ] 11 Project contribution Impact on project outcomes, helpfulness to the team, and level of engagement [ ] ✅ [ ] 12 Overall performance General assessment of performance throughout the internship period [ ] ✅ [ ] Needs Improvement Based on my self-assessment, I have identified several areas that I aim to further improve:\nTechnical competence: Although I have a solid understanding of core concepts, I still need to enhance my proficiency with certain tools and deepen my backend development expertise. Learning adaptability: While I can learn new technologies, I want to improve my ability to absorb new concepts more quickly and apply them more confidently. Initiative: I plan to be more proactive in taking ownership of tasks and suggesting improvements during project development. Communication clarity: I need to work on expressing ideas more clearly, especially when explaining technical issues or reporting progress. Collaboration: I aim to participate more actively in team discussions and contribute more consistently during group tasks. Analytical and problem-solving skills: I want to strengthen my ability to identify issues early and propose more effective, well-structured solutions. Overall performance: Although satisfactory, I see room for growth in consistency, confidence, and technical depth across all assigned responsibilities. These improvements will help me perform more effectively in future projects and continue developing as a software engineer.\n"
},
{
	"uri": "//localhost:1313/7-sharing-and-feedback/",
	"title": "Sharing and feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Suitability of Assigned Tasks\nThe tasks assigned throughout the internship aligned well with my academic background and helped me apply theoretical knowledge to real projects as well as using newly learned knowledge like AWS to the project.\n2. Quality of Guidance and Mentorship\nThe mentorship provided was supportive and clear, offering helpful explanations and allowing me to solve problems independently before receiving assistance.\n3. Learning and Skill Development\nThe internship offered valuable opportunities to learn new technologies, improve technical skills, and gain practical experience beyond what I had learned in school.\n4. Team Collaboration\nThe team demonstrated strong cooperation and communication, creating a supportive environment where members were willing to help each other when needed.\n5. Workplace Environment\nThe working environment was comfortable and positive, which helped me focus and be productive throughout the internship. However, the restroom facilities were insufficient to accommodate a large number of people.\n6. Company Culture\nThe company culture encouraged respect, teamwork, and positivity, making it easy to integrate even as a student intern.\n7. Internship Policies and Benefits\nThe internship offered a flexible work schedule. However, accessing the office required prior registration, and no additional allowances, such as for parking, were provided.\n8. Opportunities for Professional Growth\nThis internship contributed significantly to my long-term career development by offering real-world insights, industry practices, and exposure to professional workflows.\n9. Overall Satisfaction\nOverall, the internship was a valuable experience that strengthened my skills, improved my confidence, and helped define my goals as a future software engineer.\nAdditional Questions Which aspects of the internship program do you think worked well? Are there any areas of the internship program that could be improved for future interns? How do you evaluate the overall performance and contribution of interns? What support or resources could be added to help interns perform better? Do you plan to continue or expand the internship program in the future? Are there any additional suggestions or feedback for enhancing the internship experience? Suggestions \u0026amp; Expectations Improve office accessibility by simplifying the check-in process. Increase restroom capacity to better accommodate all employees and interns. Provide additional allowances, such as parking fees, to support interns. Continue offering flexible schedules while maintaining opportunities for hands-on experience and mentorship. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]