[
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerate Marketing Campaign Planning by 3x with Treasure Data AI Agents Powered by Amazon Bedrock Marketing teams face significant challenges when planning and executing campaigns across multiple channels. Traditional campaign development requires months of coordination between systems and teams for hypothesis creation, audience analysis, journey mapping, content development, activation, and measurement. This lengthy process causes brands to miss critical moments when customers are ready to engage.\nTreasure Data’s customer data platform (CDP) serves major brands globally, managing customer profiles that represent a significant portion of the internet-connected population. Working with Amazon Web Services, Inc. (AWS), the company leveraged Amazon Bedrock to create AI-powered solutions for marketing teams. Amazon Bedrock provides fully managed access to high-performing foundation models for building generative AI applications, allowing organizations to deploy AI agents that understand natural language instructions and interact with various systems autonomously.\nThis blog explores how Treasure Data’s AI-powered offerings, built on Amazon Bedrock, transform campaign creation from a months-long process into hours or days. These solutions enable marketing and CX teams to respond quickly to market opportunities and deliver personalized experiences at scale while maintaining the security and governance standards required by enterprise customers.\nBuilding AI Agents on a Foundation of Trusted Data The true power of AI lies in combining advanced foundation models with high-quality customer data. The integration between Treasure Data’s platform and Amazon Bedrock enables marketers to analyze customer data quickly, generate targeted audience segments, create detailed personas, and make data-driven decisions without requiring technical expertise. This combination reduces campaign creation time dramatically while improving targeting accuracy and campaign performance.\nCollaborative Development with AWS Treasure Data worked closely with AWS to identify key bottlenecks in traditional campaign planning and execution processes. Rather than simply adding a chat interface to existing tools, the collaboration focused on redesigning fundamental workflows to maximize AI effectiveness.\nThe partnership emphasized finding the right balance between human expertise and AI capabilities. Marketing professionals retain strategic oversight while AI agents handle time-intensive analytical tasks. This approach required building agents that could process complex data relationships and provide actionable insights grounded in actual customer behavior.\nThe collaboration resulted in a multi-agent framework built on Amazon Bedrock that addresses specific marketing challenges while maintaining the security and compliance standards required by enterprise customers.\nHow Amazon Bedrock Powers This Innovation Treasure Data selected Amazon Bedrock as the foundation for its AI agents because it enables rapid deployment without sacrificing control or security. Amazon Bedrock simplifies model selection, allowing teams to access advanced foundation models without requiring specialized data science expertise.\nThe fully managed platform enables quick deployment to production environments without building custom infrastructure from scratch. Customer data remains private and secure within the shared responsibility model between AWS and customers. AWS secures the underlying infrastructure while customers maintain control over their content and access controls.\nThe combination of Treasure Data’s customer data expertise and AI foundation models, provided by Amazon Bedrock, enables organizations to scale AI initiatives while maintaining security and governance standards.\nMeet Treasure Data’s Specialized AI Agents Treasure Data has developed several purpose-built AI agents, powered by Amazon Bedrock, to address specific marketing challenges. Each agent targets critical pain points in the campaign planning and execution process.\nThe Audience Agent enables marketers to discover and create high-value audience segments from behavioral signals quickly without needing SQL or advanced data skills. Data analysis and audience segmentation become faster and more accurate as the agent identifies patterns in customer behavior automatically. Figure 1 shows Audience Agent, which retrieves customer data based on queries. For example, when asked ‘I’d like to understand my most loyal customers,’ it identifies relevant attributes and presents the results.\nFigure 1: Audience Agent Console\nThe Deep Research \u0026amp; Analysis Agent compresses hypothesis-building processes from months to less than a week. Instead of spending extensive time on manual analysis and marketing, customer teams can generate high-quality hypotheses grounded in behavioral signals that inform strategy, testing, and execution decisions. Treasure Data’s Deep Insight Platform provides “Question Management” capability, which lets users pose questions for multiple analyses such as Churn Rate Trend and Email Performance Analysis as shown in Figure 2.\nFigure 2: Treasure Data Deep Insight Platform\nAvailable as part of Treasure Data’s CDP Trade-Up program, the Migration Agent accelerates transitions from existing customer data platforms by up to 60%. It extracts queries, segments, and transformation logic from current systems and automatically generates SQL, pipelines, and orchestration workflows. This agent helps organizations preserve their segments, workflows, and business logic when moving data, avoiding the need to start from scratch.\nThese agents use retrieval augmented generation (RAG), combining data processing capabilities, with the power of Amazon Bedrock inference, to provide accurate and data-grounded responses. This ensures AI suggestions reflect actual customer behavior rather than generic recommendations.\nIntroducing Treasure Data’s AI Agent Foundry While the pre-built agents address common marketing challenges, Treasure Data customers expressed the need to create customized agents tailored to their unique business requirements and industry-specific use cases. The AI Agent Foundry emerged as the solution to this demand.\nThe AI Agent Foundry serves as the foundation for building custom AI agents tailored to specific business needs. Marketing, customer experience, and data teams can create, refine, and deploy agents without deep technical knowledge. High-impact use cases could include journey orchestration, data health monitoring, and campaign optimization specific to their organization.\nThe Foundry includes built-in security features, permission controls, auditability, and access management that meet enterprise governance requirements. Organizations can experiment with AI capabilities and deploy agents while maintaining data security, privacy and regulatory compliance. This approach enables customers to build agents that address their specific market dynamics and business processes.\nPractical Applications Driving Results The specialized agents address several critical marketing use cases through their integration with Amazon Bedrock. Decision support helps marketers evaluate multiple factors simultaneously when determining campaign targeting, messaging, and channel selection. The AI provides recommendations based on comprehensive data analysis rather than intuition alone.\nMultiple team members can collaborate with AI agents simultaneously, democratizing access to customer insights across marketing organizations. This capability eliminates bottlenecks caused by limited technical expertise on marketing teams.\nThe agents continuously learn from customer interactions and campaign performance, enabling organizations to refine their approach and achieve better results through rapid iteration and optimization.\nReal-World Impact: Nobitel Case Study Nobitel Co., Ltd., a leader in health and sports services, operates Dr. Stretch, a specialized stretching chain with 240+ locations across Japan. The company faced challenges with its marketing operations, where manual campaign planning and data silos prevented non-technical teams from accessing customer insights and delivering timely personalized offers.\nTo address these challenges, Nobitel implemented Treasure Data’s AI Agent Foundry, built on AWS AI/ML services including Amazon Bedrock. This implementation transformed their marketing operations, enabling non-technical marketers to execute personalized campaigns without advanced data skills. The results included 3x faster campaign planning and 20% improvement in store efficiency. Learn more about Nobitel’s transformation in their case study.\nThe Future of AI-Powered Marketing AI agents represent the beginning of a transformation that will reshape marketing and customer experience operations. Future developments will see agents testing messaging variations, generating creative content, orchestrating multi-channel campaigns, and optimizing spending in real-time across devices and regions.\nMarketing and CX professionals will evolve from campaign executors to strategic orchestrators. The critical question becomes whether data infrastructure can support numerous autonomous campaigns running simultaneously with precision and control.\nThis future requires robust data foundations, sophisticated AI capabilities, and governance frameworks that ensure trust and compliance at scale. Organizations building this infrastructure today position themselves to capitalize on autonomous marketing and CX operations.\nTransforming Marketing Through AI and Data Treasure Data’s specialized AI agents and AI Agent Foundry, powered by Amazon Bedrock, represent a fundamental shift in how marketing, CX, and data teams drive value from customer data. By combining trusted data with advanced foundation models, teams can analyze data, create segments, generate personas, and make strategic decisions in hours rather than months.\nThis transformation democratizes access to customer insights and automates complex analytical tasks. Marketing teams can respond faster to market opportunities and achieve better results through rapid iteration. The solution demonstrates that effective marketing requires both intelligent agents and the robust data infrastructure that makes them truly powerful.\nSecurity and compliance remain a shared responsibility between AWS and customers. AWS provides a secure, compliant foundation through Amazon Bedrock, while customers maintain control over their data and access policies. This approach enables organizations to innovate with AI while meeting their governance requirements.\nConclusion Treasure Data’s AI Agent Foundry and pre-built AI agents, powered by Amazon Bedrock, transform marketing campaign creation from a months-long process to just hours or days. These AI solutions enable marketers to quickly analyze data, create segments, generate personas, and make data-driven decisions without deep technical expertise. By democratizing access to customer insights and automating complex analytical tasks, all powered by Amazon Bedrock’s foundation models, marketing teams can now respond faster to market opportunities and achieve better results through rapid iteration.\nTreasure Data – AWS Partner Spotlight Treasure Data, an AWS partner, Treasure Data is the Intelligent Customer Data Platform purpose-built for enterprise scale. Trusted by Yum! Brands, Stellantis, AXA, and over 80 Global 2000 companies, Treasure Data is where trust, performance, and AI-first architecture converge to drive revenue with hyper-personalized customer experiences, lower marketing costs, and reduce risk. Treasure Data provides both out-of-the-box agents and the AI Agent Foundry that enables data-driven teams or partners to utilize, create, and deploy AI Agents on the Treasure Data platform and across their workflows while leveraging their data within the trusted Treasure Data environment.\nRonak Shah Ronak Shah is a Principal Partner Solution Architect with the AWS Industry Vertical team based in the New York City area. He works with AWS Partners in the Retail and CPG verticals to co-innovate on AWS. He is interested in finding new trends in retail and building innovative solutions in the areas of digital commerce, supply chain, customer experience and marketing technology. Outside of work, he volunteers at scouting and local debate competitions.\rHiroshi Nakamura Hiroshi Nakamura is an accomplished technology leader with extensive experience in software engineering and system architecture. Currently serving as CTO and VP of Engineering at Treasure Data since October 2014, Hiroshi has been instrumental in designing and developing a cloud-based Data Management platform capable of handling immense data volumes. An active open-source developer since April 1999, contributions include significant enhancements to Ruby and JRuby. Hiroshi holds a Master's degree in Science and Engineering from Waseda University.\rPranjal Gururani Pranjal Gururani is a Solutions Architect at AWS based out of Seattle. Pranjal works with various customers to architect cloud solutions that address their business challenges. He enjoys hiking, kayaking, skydiving, and spending time with family during his spare time.\r"
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Build resilient generative AI agents Generative AI agents in production environments demand resilience strategies that go beyond traditional software patterns. AI agents make autonomous decisions, consume substantial computational resources, and interact with external systems in unpredictable ways. These characteristics create failure modes that conventional resilience approaches might not address.\nThis post presents a framework for AI agent resilience risk analysis that applies to most AI developments and deployment architectures. We also explore practical strategies to help prevent, detect, and mitigate the most common resilience challenges when deploying and scaling AI agents.\nGenerative AI agent resilience risk dimensions To identify resilience risks, we break down the generative AI agent systems into seven dimensions:\nFoundation models – Foundation models (FMs) provide core reasoning and planning capabilities. Your deployment choice determines your resilience responsibilities and costs. The three deployment approaches are fully self-managed such as using Amazon Elastic Compute Cloud (Amazon EC2), server-based managed services such as using Amazon SageMaker AI, or serverless managed services such as Amazon Bedrock. Agent orchestration – This component controls how multiple AI agents and tools coordinate to achieve complex goals, containing logic for tool selection, human escalation triggers, and multi-step workflow management. Agent deployment infrastructure – The infrastructure encompasses the underlying hardware and system where agents run. The infrastructure options include using fully self-managed EC2 instances, managed services such as Amazon Elastic Container Services (Amazon ECS), and specialized managed services designed specifically for agent deployment, such as Amazon Bedrock AgentCore Runtime. Knowledge base – The knowledge base includes vector database storage, embedding models, and data pipelines that create vector embeddings, forming the foundation for Retrieval Augmented Generation (RAG) applications. Amazon Bedrock Knowledge Bases supports fully managed RAG workflows. Agent tools – This includes API tools, Model Context Protocol (MCP) servers, memory management, and prompt caching features that extend agent capabilities. Security and compliance – This component encompasses user and agent security controls as well as content compliance monitoring, supporting proper authentication, authorization, and content validation. Security includes inbound authentication that manages users’ access to agents, and outbound authentication and authorization that manages agents’ access to other resources. Outbound authorization is more complex because agents might require their own identity. Amazon Bedrock AgentCore Identity is the identity and credential management service designed specifically for AI agents, providing inbound and outbound authentication and authorization capabilities. To help prevent compliance violations, organizations should establish comprehensive responsible AI policies. Amazon Bedrock Guardrails provides configurable safeguards for responsible AI policy implementation. Evaluation and observability – These systems track metrics from basic infrastructure statistics to detailed AI-specific traces, including ongoing performance evaluation and detection of behavioral deviations. Agent evaluation and observability requires a combination of traditional system metrics and agent-specific signals, such as reasoning traces and tool invocation results. The following diagram illustrates these dimensions.\nThis configuration provides visibility into agent applications, enabling subsequent sessions to deliver targeted resilience analysis and mitigation recommendations.\nTop 5 resilience problems for agents and mitigation plans The Resilience Analysis Framework defines fundamental failure modes that production systems should avoid. In this post, we identify generative AI agents’ five primary failure modes and provide strategies that can help establish resilient properties.\nShared fate Shared fate occurs when a failure in one agent component cascades across system boundaries, affecting the entire agent. Fault isolation is the desired property. To achieve fault isolation, you must understand how agent components interact and identify their shared dependencies.\nThe relationship between FMs, knowledge bases, and agent orchestration requires clear isolation boundaries. For example, in RAG applications, knowledge bases might return irrelevant search results. Implementing guardrails with relevance checks can help prevent these query errors from cascading through the rest of the agent workflow.\nTools should align with fault isolation boundaries to contain impact in case of failure. When building custom tools, design each tool as its own containment domain. When using MCP servers or existing tools, make sure you use strict, versioned request/response schemas and validate them at the boundary. Add semantic validations such as date ranges, cross-field rules, and data freshness checks. Internal tools can also be deployed across different AWS Availability Zones for additional resilience.\nAt the orchestration dimension, implement circuit breakers that monitor failure rates and latency, activating when dependencies become unavailable. Set bounded retry limits with exponential backoff and jitter to control cost and contention. For connectivity resilience, implement robust JSON-RPC error mapping and per-call timeouts, and maintain healthy connection pools to tools, MCP servers, and downstream services. The orchestration dimension should also manage contract-compatible fallbacks—routing from a failed tool or MCP server to alternatives—while maintaining consistent schemas and providing degraded functionality.\nWhen isolation boundaries fail, you can implement graceful degradation that maintains core functionality while advanced features become unavailable. Conduct resilience testing with AI-specific failure injection, such as simulating model inference failures or knowledge base inconsistencies, to test your isolation boundaries before problems occur in production.\nInsufficient capacity Excessive load can overwhelm even well-provisioned systems, potentially leading to performance degradation or system failure. Sufficient capacity makes sure your systems have the resources needed to handle both expected traffic patterns and unexpected surges in demand.\nAI agent capacity planning involves demand forecasting, resource assessment, and quota analysis. The primary consideration when planning capacity is estimating Requests Per Minute (RPM) and Tokens Per Minute (TPM). However, estimating RPM and TPM presents unique challenges due to the stochastic nature of agents. AI agents typically use recursive processing, where the agent’s reasoning engine repeatedly calls the FMs until reaching final answers. This creates two major planning difficulties. First, the number of iterative calls is hard to predict because it’s based on task complexity and reasoning paths. Second, each call’s token length is also hard to predict because it includes the user prompt, system instructions, agent-generated reasoning steps, and conversation history. This compounding effect makes capacity planning for agents difficult.\nThrough heuristic analysis during development, teams can set a reasonable recursion limit to help prevent redundant loops and runaway resource consumption. Additionally, because agent outputs become inputs for subsequent recursions, managing maximum completion tokens helps control one component of the growing token consumption in recursive reasoning chains.\nThe following equations help translate agent configurations to these capacity estimates:\nRPM = Average agent level thread per minute * average FM invocation per minute in one thread = Average agent level thread per minute * (1 + 60/(max_completion_tokens/TPS))\nToken per second (TPS) is different for each model, and can be found in model release documentation and open source benchmark results, such as artificial analysis.\nTPM = RPM * Average input token length = RPM * (system prompt length + user prompt length + max_completion_tokens * (recursion_limit -1)/recursion_limit)\nThis calculation is assuming no prompt caching feature is implemented.\nUnlike external tools where resilience is managed by third-party providers, internally developed tools rely on proper configuration by the development team to scale based on demand. When resource needs spike unexpectedly, only the affected tools require scaling.\nFor example, AWS Lambda functions can be converted to MCP-compatible tools using Amazon Bedrock AgentCore Gateway. If popular tools cause Lambda functions to reach capacity limits, you can increase the account-level concurrent execution limit or implement provisioned concurrency to handle the increased load.\nFor scenarios involving multiple action groups executing simultaneously, Lambda functions’ reserved concurrency controls provide essential resource isolation by allocating dedicated capacity to each action group. This helps prevent a single tool from consuming all available resources during orchestrated invocations, facilitating resource availability for high-priority functions.\nWhen capacity limits are reached, you can use intelligent request queuing with priority-based allocation to make sure essential services continue operating. Implementing graceful degradation during high-load periods can be helpful. This maintains core functionality while temporarily reducing non-essential features.\nExcessive latency Excessive latency compromises user experience, reduces throughput, and undermines the practical value of AI agents in production. Agentic workload development requires balancing speed, cost, and accuracy. Accuracy is the cornerstone for AI agents to gain user trust. Achieving high accuracy requires allowing agents to perform multiple reasoning iterations, which inevitably creates latency challenges.\nManaging user expectations becomes critical—establishing service level objective (SLO) metrics before project initiation sets realistic targets for agent response times. Teams should define specific latency thresholds for different agent capabilities, such as subsecond responses for simple queries vs. longer windows for analytical tasks requiring multiple tool interactions or extensive reasoning chains. Clear communication of the expected response times helps prevent user frustration and allows for appropriate system design decisions.\nPrompt engineering offers the greatest opportunity for latency improvement by reducing unnecessary reasoning loops. Vague prompts take agents into extensive deliberation cycles, whereas clear instructions accelerate decision-making. Asking an agent to “approve if the use case is of strategic value” creates a complex reasoning chain. The agent must first define strategic value criteria, then evaluate which criteria apply, and finally determine significance thresholds. Conversely, clearly stating the criteria in the system prompt can largely reduce agent iterations. The following examples illustrate the difference between ambiguous and clear instructions.\nThe following is an example of an ambiguous agent instruction:\nYou are a generative AI use case approver.\nYour role is to evaluate GenAI agent build requests by carefully analyzing user-provided information and make approval decisions. Please follow the following instructions:\n\u0026lt;instructions\u0026gt;\nCarefully analyze the information provided by the user, and collect use case information, such as use case sponsor, significance of the use case, and potential values that it can bring.\nApprove the use case if it has a senior sponsor and is of strategic value.\n\u0026lt;/instructions\u0026gt;\nThe following is an example of a clear, well-defined agent instruction:\nYou are a generative AI use case approver.\nYour role is to evaluate Gen AI agent build requests by carefully analyzing user-provided information and make approval decisions based on specific criteria.\nPlease strictly follow the following instructions:\n\u0026lt;instructions\u0026gt;\nCarefully analyze the information provided by the user. Collect answers to the following questions:\n\u0026lt;question_1\u0026gt;Does the use case have a business sponsor that is VP level and above?\u0026lt;/question_1\u0026gt;\n\u0026lt;question_2\u0026gt;What value is this agent expected to deliver? The answer can be in the form of\nnumber of hours per month saved on certain tasks, or additional revenue values.\u0026lt;/question_2\u0026gt;\n\u0026lt;question_3\u0026gt;If the use case is external customer facing, please provide supporting information\non the demand.\u0026lt;/question_3\u0026gt;\nEvaluate the request against these approval criteria:\n\u0026lt;criteria_1\u0026gt;The use case has business sponsor at VP level and above. This is a hard criteria.\u0026lt;/criteria_1\u0026gt;\n\u0026lt;criteria_2\u0026gt;The use case can bring significant $ value, calculated by productivity gain or\nrevenue increase. This is a soft criteria.\u0026lt;/criteria_2\u0026gt;\n\u0026lt;criteria_3\u0026gt;Have strong proof that the use case/feature is demanded by customers. This is a\nsoft criteria.\u0026lt;/criteria_3\u0026gt;\nBased on the evaluation, make a decision to approve or deny the use case.\nApprove: If the hard criterion is met, and at least one of the soft criteria is met. Deny: The hard criterion is not met, or neither of the soft criteria is met. \u0026lt;/instructions\u0026gt;\nPrompt caching delivers substantial latency reductions by storing repeated prompt prefixes between requests. Amazon Bedrock prompt caching can reduce latency by up to 85% for supported models, particularly benefiting agents with long system prompts and contextual information that remains stable across sessions.\nAsynchronous processing for agents and tools reduces latency by enabling parallel execution. Multi-agent workflows achieve dramatic speedups when independent agents execute in parallel rather than waiting for sequential completion. For agents with tools, asynchronous processing enables continued reasoning and preparation of subsequent actions while tools execute in the background, optimizing workflow by overlapping cognitive processing with I/O operations.\nSecurity and compliance checks must minimize latency impact while maintaining protection across dimensions. Content moderation agents implement streaming compliance scanning that evaluates agent outputs during generation rather than waiting for complete responses, flagging potentially problematic content in real time while allowing safe content to flow through immediately.\nIncorrect agent response Correct output makes sure your AI agent performs reliably within its defined scope, delivering accurate and consistent responses that meet user expectations and business requirements. However, misconfiguration, software bugs, and model hallucinations can compromise output quality, leading to incorrect responses that undermine user trust.\nTo improve accuracy, use deterministic orchestration flows whenever possible. Letting agents rely on LLMs to improvise their way through tasks creates opportunities to deviation from your intended path. Instead, define explicit workflows that specify how agents should interact and sequence their operations. This structured approach reduces both inter-agent calling errors and tool-calling mistakes. Additionally, implementing input and output guardrails significantly enhances agent accuracy. Amazon Bedrock Guardrails can scan user input for compliance checks before model invocations, and provide output validation to detect hallucinations, harmful responses, sensitive information, and blocked topics.\nWhen response quality issues occur, you can deploy human-in-the-loop validation for high-stakes decisions where accuracy is essential, and implement automatic retry mechanisms with refined prompts when initial responses don’t meet quality standards.\nSingle point of failure Redundancy creates multiple paths to success by minimizing single points of failure that can cause system-wide impairments. Single points of failure undermine redundancy when multiple components depend on a single resource or service, creating vulnerabilities that bypass protective boundaries. Effective redundancy requires both redundant components and redundant pathways, making sure that if one component fails, alternative components can take over, and if one pathway becomes unavailable, traffic can flow through different routes.\nAgents require coordinated redundancy for their FMs. If the models are self-managed, you can implement multi-Region model deployment with automated failover. When using managed services, Amazon Bedrock offers cross-Region inference to provide built-in redundancy for supported models, automatically routing requests to alternative AWS Regions when primary endpoints experience issues.\nThe agent tools dimension must coordinate tool redundancy to facilitate graceful degradation when primary tools become unavailable. Rather than failing entirely, the system should automatically route to alternative tools that provide similar functionality, even if they’re less sophisticated. For example, when the internal chat assistant’s knowledge base fails, it can fall back to a search tool to deliver alternative output to users.\nMaintaining permission consistency across redundant environments is essential. This helps prevent security gaps during failover scenarios. Because overly permissive access controls pose significant security risks, it’s critical to validate that both end-user permissions and tool-level access rights are identical between primary and failover components. This consistency makes sure security boundaries are maintained regardless of which environment is actively serving requests, helping prevent privilege escalation or unauthorized access that could occur when systems switch between different permission models during operational transitions.\nOperational excellence: Integrating traditional and AI-specific practices Operational excellence in agentic AI integrates proven DevOps practices with AI-specific requirements for running agentic systems reliably in production. Continuous integration and continuous delivery (CI/CD) pipelines orchestrate the full agent lifecycle, and infrastructure as code (IaC) standardizes deployments across environments, reducing manual error and improving reproducibility.\nAgent observability requires a combination of traditional metrics and agent-specific signals such as reasoning traces and tool invocation results. Although traditional system metrics and logs can be obtained from Amazon CloudWatch, agent-level tracing requires additional software build. The recently announced Amazon Bedrock AgentCore Observability (preview) supports OpenTelemetry to integrate agent telemetry data with existing observability services, including CloudWatch, Datadog, LangSmith, and Langfuse. For more details the Amazon Bedrock AgentCore Observability features, see Launching Amazon CloudWatch generative AI observability (Preview).\nBeyond monitoring, testing and validation of agents also extend beyond conventional software practices. Automated test suites such as promptfoo help development teams configure tests to evaluate reasoning quality, task completion, and dialogue coherence. Pre-deployment checks confirm tool connectivity and knowledge access, and fault injection simulates tool outages, API failures, and data inconsistencies to surface reasoning flaws before they affect users.\nWhen issues arise, mitigation relies on playbooks covering both infrastructure-level and agent-specific issues. These playbooks support live sessions, enabling seamless handoffs to fallback agents or human operators without losing context.\nSummary In this post, we introduced a seven-dimension architecture model to map your AI agents and analyze where resilience risks emerge. We also identified five common failure modes related to AI agents, and their mitigation strategies.\nThese strategies illustrated how resilience principles apply to common agentic workloads, but they are not exhaustive. Each AI system has unique characteristics and dependencies. You must analyze your specific architecture across the seven risk dimensions to identify the resilience challenges within your own workloads, prioritizing areas based on user impact and business criticality rather than technical complexity.\nResilience represents an ongoing journey rather than a destination. As your AI agents evolve and handle new use cases, your resilience strategies must evolve accordingly. You can establish regular testing, monitoring, and improvement processes to make sure your AI systems remain resilient as they scale. For more information about generative AI agents and resilience on AWS, refer to the following resources:\nChaos Engineering Scenarios for GenAI workloads Designing generative AI workloads for resilience Introducing Amazon Bedrock AgentCore: Securely deploy and operate AI agents at any scale (preview) Implement effective data authorization mechanisms to secure your data used in generative AI applications: Part 1 and Part 2 Yiwen Zhang Yiwen Zhang is a Principal GenAI Solutions Architect at AWS. She develops both generative AI applications and enterprise integration strategies. With a PhD in computational statistics and a decade of experience developing and deploying AI/ML and data pipelines, she brings deep technical expertise to enterprise AI transformation. She focuses on making generative AI practical by establishing successful generative AI MVPs and addressing challenging questions such as ROI, AI agent performance tuning, cost management, security, and production resilience. Her goal is to help enterprises build generative AI agents into “outcome as a service” solutions.\rHechmi Khelifi Hechmi Khelifi is an Enterprise Solutions Architect at AWS, focusing on resilience and reliability. With 3+ years at AWS and a PhD from the University of Quebec, Hechmi leverages his extensive IT experience and strong academic background to help customers build robust and resilient solutions.\rJennifer Moran Jennifer Moran is an AWS Senior Resilience Specialist Solutions Architect. She brings a wealth of experience from her diverse technical background, encompassing various roles across the software industry. Her expertise focuses on helping customers design resilient solutions to improve their overall resilience posture.\r"
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Seamlessly burst EDA jobs to AWS using Synopsys Cloud Hybrid solution This post was contributed by Varun Shah, Xingang Zhao (Synopsys), Dnyanesh Digraskar, Mayur Runwal (AWS)\nIn this post we describe how customers can burst their Synopsys workloads to AWS using Synopsys Cloud Hybrid Solution. This simplifies the workflow by eliminating the need to manually move data between on-premises storage and the cloud. Data required by a job running on AWS is incrementally cached in the background and made available to the process. Similarly, when needed, selective data that’s written by a job running in the cloud can be pushed to on-premises in real time – keeping the on-premises storage up to date.\nThe semiconductor industry is experiencing explosive growth, driven by artificial intelligence (AI), high performance computing (HPC), and the proliferation of smart systems. Engineers are facing unprecedented complexity and an increasingly rapid pace of innovation. As chips become more powerful and intricate, design cycles grow longer, costs rise, and the risk of missing market windows increases. In this environment, time to market has become a critical differentiator.\nCloud-based infrastructure offers elasticity and flexibility to scale chip design operations and meet customer needs. AWS offers easy access to cloud compute resources which are optimized for EDA workloads. Amazon Elastic Compute Cloud (Amazon EC2) provides access to the latest generation processors in virtually unlimited capacity, deployable across the globe. Latest generation of x86 and ARM-based instances on AWS help realize maximum performance and let customers squeeze the most out of their EDA software licenses.\nSynopsys is an AWS Partner delivering trusted and comprehensive silicon to systems design solutions for customers, from EDA to silicon IP and system verification and validation.\nIn this post we’ll demonstrate scale-testing a set of Synopsys EDA tools on AWS and describe performance, quality of results, and turnaround time.\nSynopsys Cloud Hybrid Solution: bursting EDA jobs to AWS AWS makes it easy to setup cloud compute infrastructure with its large selection of hardware and services for semiconductor and hi-tech flows. Companies can match their compute needs with those of their time-critical workflows. At a high-level, the steps involved in bursting EDA workloads on AWS involve:\nIdentifying the design blocks/jobs that need to be run on the cloud Transferring relevant data to the cloud Installing EDA application binaries and licenses on the cloud Running EDA jobs Transferring the results back to on-premises for analysis/debug/reporting Steps 1 \u0026amp; 2 above can be cumbersome as packaging the right set of files, source code, libraries, dependencies, scripts etc. can take up to several weeks of time. Synopsys Cloud Hybrid Solution provides a cloud-enabled mechanism to accomplish these tasks more easily.\nArchitecture and Components Figure 1: Architectural diagram of Synopsys Cloud Hybrid solution with AWS used for testing described in this blog\nFigure 1 shows the high-level architecture of the Synopsys Cloud Hybrid solution. The key architectural components include:\nOn-premises storage: These are the NFS mount points with design data that must be presented to the compute nodes on AWS. The hybrid solution works with any third-party or commercial ISV storage solutions.\nScheduler: The Hybrid solution works with any commercially available job schedulers like the ones in AWS Parallel Computing Service (PCS), AWS ParallelCluster, AWS Batch, or IBM LSF, UGE, Slurm, etc. The architecture diagram illustrates the use of IBM LSF multi-cluster which integrates with LSF resource connector for provisioning resources on AWS.\nCompute farms:\nOn-premises: This is the datacenter with a mix of several different types of compute servers. On AWS: The Amazon EC2 instances used for performing the EDA simulations. For this testing, we used 16xlarge instances with 3.5GHz CPU, with 8GB memory per vCPU. A broad range of EC2 instance types could be used, depending on the application and semiconductor design under question. On Cloud NFS: For EDA jobs which write intermediate data that isn’t required on-premises, an Amazon FSx for NetApp ONTAP filesystem was provisioned for use by compute resources on AWS.\nSynopsys Cloud Hybrid Solution: Users install this binary on their Amazon EC2 instance and configure the data-sync to handle data movement between on-prem and AWS.\nLicensing:\nLicenses for EDA applications: Existing EDA application licenses and license servers continue to serve licenses for jobs running on AWS. Any additional license needs can be served by Synopsys FlexEDA and Pay-Per-Use metered licensing. License for Hybrid solution: This is a separate application that needs to be purchased by the users from Synopsys. The licensing is served from Synopsys managed network license servers. Setting up the Synopsys Cloud Hybrid Solution The setup includes 3 main components:\nHybrid Cloud data-sync: This entails installing the Hybrid solution on a recommended instance type on AWS, configure networking to allow traffic across on-premises and AWS resources for licensing (of Hybrid Cloud solution), data storage and distributed processing, identifying NFS storage mounts on-prem that require mapping and starting the Hybrid solution to establish the data-sync mapping. Network Connectivity: Dedicated network connections like AWS DirectConnect provide higher bandwidth, lower latency which makes it ideal for handling workloads with large data transfers. Alternatively, IPsec Virtual Private Network (VPN) tunnel can be used with AWS but offers less speed and reliability compared to dedicated network connections. Scheduler setup: This is specific to every scheduler. For example, in the case of IBM LSF, this involves deploying LSF multi-cluster, defining participating clusters, configuring queues and verifying that clusters on AWS can be dynamically provisioned as needed. General information about the platform With , selective data that is written by a job running on cloud, can be written back to on-premises in real time, thereby keeping the on-premises storage up to date. Note that data write-back incurs data egress charges from AWS. Temporary output data from running jobs that’s not required on-prem or large persistent output data can be directed to an NFS on-cloud. If necessary, this data can be scripted to write-back after job completion.\nThe job scheduler should be provisioned by the customer. It then finds an appropriate compute resource, on-premises or on AWS, and the Hybrid Solution handles the data movement necessary. Synopsys Cloud Hybrid Solution has ability to configure the scheduler to automatically exhaust on-premises compute capacity first and then schedule jobs on-cloud compute nodes, or to run all jobs on cloud compute nodes depending on the application being run and size/frequency of data being shared between different workers of the application.\nScale-testing Synopsys Hybrid Solution on AWS EDA jobs can be data intensive both in terms of input and output. The latency of the network and the amount of data moving back and forth can impact the performance of EDA jobs. To understand the boundaries of the Synopsys Hybrid Solution, we used six of Synopsys’ flagship applications which are essential for every semiconductor chip design process. These are:\nPrimeLib – a library characterization and validation application Synopsys VCS – Simulator for verifying semiconductor designs PrimeSim SPICE – GPU-accelerated SPICE Simulator for Analog, RF, and - Mixed-signal Design Synopsys PrimeWave – application for simulation setup and analysis of analog, RF, mixed-signal design, custom-digital and memory designs PrimeTime – static timing analysis application with fast, memory-efficient scalar and multicore computing Synopsys DSO.ai – an AI-powered design space optimization application All these EDA applications have different profiles in terms of I/O patterns, amount of data generated (size and volume of files), and the amount and frequency of communication that happens between different workers of a job. Each of these applications were executed using the Synopsys Cloud Hybrid Solution with standard designs and testcases. We prioritized jobs that require high IOPs when selecting the testcases for the benchmarking exercise.\nWe monitored several metrics and compared them between tests performed on Synopsys Cloud Solution on AWS and comparable on-premises infrastructure:\nPerformance: Runtime performance is a critical metric as it directly translates to time to market. Quality of Results (QoR): QoR is another critical metric that cannot be compromised on. The applications analyze the results in terms of coverage, passing tests and regressions and the output their interpretation of the overall quality of results. For the applications tested, we expect to see same or similar QoR results between AWS and on-premises tests. Setup time: If performance and QoR are reasonably on-par, then what makes Synopsys Cloud Hybrid Solution worthwhile is the time that can be saved in moving relevant data to the cloud to use AWS compute infrastructure. While it’s hard to measure this on-premises and differs greatly from one EDA flow to another, we tried to estimate this. Results Performance Table 1: Performance of Synopsys applications run on Hybrid Solution with AWS vs on-premises\nWe ran this baseline of tests either on-premises or on AWS depending on resource availability on premises. Table 1 shows that the overall performance of all EDA flows tested using Synopsys Cloud Hybrid Solution was comparable to – or in some cases, better than – the baseline. In the case of Hybrid Setup, the runtime includes time taken to write final output data back to on-premises storage through the Hybrid Solution. This impacts the overall performance slightly when compared to a baseline run natively on cloud like in the case of PrimeLib and VCS. The data also shows that performance was better than a baseline run on-premises for PrimeTime and PrimeSim. This is due to the availability of faster CPUs on AWS.\nThrough understanding IO patterns (size, frequency) of different EDA applications, we identified some best practices for setting up these applications to best use the Hybrid Solution.\nQuality of Results (QoR) As expected, there was no degradation in quality of results, PPA or coverage achieved when using the Hybrid Solution. For EDA applications like VCS, PrimeSim, results matched exactly with the runs on-premises. For other EDA applications like DSO.ai, the reports, PPA and QoR is comparable.\nSetup Time: Synopsys Hybrid Cloud saves up to several weeks of setup time per project by automating the manual data movement and verification of design blocks. For example, from our own internal testing of an EDA application’s QA regressions, we know that ~4 weeks of manual effort was saved because Hybrid Solution eliminated the need to lift and shift EDA workloads.\nDepending on the workflows, Synopsys can help in best practice recommendations on a cloud-native mode or a burst mode with Synopsys Cloud Hybrid Solution. With the true burst mode, data is written to on-cloud NFS, and then selective data is synced back to on-premises after the job completion. In this way, data egress and performance impacts are minimized. Burst mode adds huge value by incrementally moving input data as needed to the cloud for running the jobs through Hybrid Solution.\nConclusion EDA workloads demand immense computational power and seamless data access. The sheer volume of design data, coupled with the iterative and highly interdependent nature of EDA tasks, means that constant data movement between on-premises storage and cloud compute can become a slow and inefficient process.\nSynopsys Cloud Hybrid Solution solves this problem by providing a seamless way to burst EDA jobs from on-premises to the cloud with real-time two-way data synchronization. The product was stress-tested with six of Synospys’ flagship EDA applications. We found that performance of applications in Hybrid mode was comparable to performance of an on-premises or cloud-native run all while maintaining the same Quality of Results. Data movement was invisibly handled by Synopsys Cloud Hybrid Solution which saved up to weeks of setup time.\nVarun Shah Varun Shah is on the product management team for Synopsys Cloud, responsible for technology roadmap, cloud enablement of EDA software and voice-of-customer analysis. Varun holds a master's degree in electrical engineering from the University of Missouri-Rolla and is a certified product management professional from the Kellogg School of Management.\rDnyanesh Digraskar Dnyanesh Digraskar is a Principal HPC Partner Solutions Architect at AWS. He leads the HPC implementation strategy with AWS ISV partners to help them build scalable well-architected solutions. He has more than fifteen years of expertise in the areas of CFD, CAE, Numerical Simulations, and HPC. Dnyanesh holds a Master’s degree in Mechanical Engineering from University of Massachusetts, Amherst.\rMayur Runwal Mayur Runwal is a Senior Solutions Architect at AWS specializing in Electronic Design Automation (EDA). He architects semiconductor design and verification workflows for AWS customers. Before joining AWS, he led IT infrastructure teams at semiconductor companies for 10 years, where he designed and implemented virtual desktop solutions. His expertise includes high-performance computing, cloud architecture, and enterprise IT solutions.\rXingang Zhao Xingang Zhao is a Technical Product Manager for Synopsys Cloud. He's responsible for the technical assessment and integration of emerging cloud services and tools on public cloud platforms to boost the performance and cost-effectiveness of EDA workloads. Xingang holds a BS in electrical engineering from Zhejiang University and is a certified product management professional from the Kellogg School of Management.\r"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.1-createvpc/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": "Create VPC Lab VPC Go to VPC service management console Click Your VPC. Click Create VPC. At the Create VPC page. In the Name tag field, enter Lab VPC. In the IPv4 CIDR field, enter: 10.10.0.0/16. Click Create VPC. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/",
	"title": "Preparing VPC and EC2",
	"tags": [],
	"description": "",
	"content": "In this step, we will need to create a VPC with 2 public / private subnets. Then create 1 EC2 Instance Linux located in the public subnet, 1 EC2 Instance Windows located in the private subnet.\nThe architecture overview after you complete this step will be as follows:\nTo learn how to create EC2 instances and VPCs with public/private subnets, you can refer to the lab:\nAbout Amazon EC2 Works with Amazon VPC Content Create VPC Create Public Subnet Create Private Subnet Create security group Create public Linux server Create private Windows server "
},
{
	"uri": "//localhost:1313/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "\rINTERNSHIP REPORT Student Information Full Name: Lương Nguyễn Duy Khang\nPhone Number: 0931984914\nEmail: lndkhang278@gmail.com\nUniversity: FPT University\nMajor: Software Engineer\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback Content Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/4-s3log/4.1-updateiamrole/",
	"title": "Update IAM Role",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "//localhost:1313/1-introduce/week1/",
	"title": "WEEK 1",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 1 objectives: Connected and formed groups with students at FCJ Proposed ideas about final project Studied and did labs on FCJ websites Understood basic concepts of cloud and AWS services Task carried out this week Day Task Start Date Completion Date Reference Material 2 - Get used to and take notes of First Cloud Journey tasks for students, its rules and learning materials 09/08/2025 09/08/2025 3 - Self study about cloud - Explore AWS services 09/09/2025 09/09/2025 Link 4 - Create AWS Free Tier account - Learn how to manage cost with AWS Budgets Do: + Create AWS account + Create different types of budgets 09/10/2025 09/10/2025 Link 5 - Study about EC2 - SSH connection methods to EC2 - Learn about Elastic IP - Propose ideas about final project - Study about VPC 09/11/2025 09/11/2025 Link 6 Do: + Create security groups + Create subnets + Launch an EC2 instance + Connect via SSH + Attach an EBS volume - Join a meeting and choose one idea for final project 09/12/2025 09/12/2025 Link Week 1 Achievements Understood what AWS is and learned the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Explored AWS services through self-study and documentation to understand their usage.\nLearned how to manage costs effectively with AWS Budgets, including creating different types of budgets.\nGained hands-on experience with EC2 by studying its features, connection methods (SSH), and Elastic IP.\nLearned the basics of VPC and networking setup in AWS.\nPracticed creating security groups, subnets, and launching an EC2 instance.\nSuccessfully connected to an EC2 instance via SSH and attached an EBS volume.\nWorked collaboratively to propose and select one idea for the final project.\n"
},
{
	"uri": "//localhost:1313/1-introduce/week2/",
	"title": "WEEK 2",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 2 Objectives Learn and practice AWS storage and database services (Amazon S3, Amazon RDS). Get familiar with and self-study Spring Boot to prepare for backend development. Gain knowledge of system monitoring with Amazon CloudWatch. Learn and practice deployment with Amazon Lightsail and Lightsail Container. Continue self-studying PostgreSQL to integrate with Spring Boot. Explore resource scaling with Amazon EC2 Auto Scaling. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Study Amazon S3 and Amazon RDS\n- Do: Amazon S3 labs, Amazon RDS labs\n- Self-study Spring Boot 09/15/2025 09/15/2025 AWS Study Group 3 - Self-study Spring Boot\n- Study Amazon CloudWatch 09/16/2025 09/16/2025 AWS Study Group 4 - Self-study Spring Boot and PostgreSQL\n- Do: Amazon CloudWatch labs 09/17/2025 09/17/2025 AWS Study Group 5 - Self-study Spring Boot and PostgreSQL\n- Study Amazon Lightsail and Lightsail Container\n- Do: Lightsail labs, Lightsail Container labs 09/18/2025 09/18/2025 AWS Study Group 6 - Self-study Spring Boot and PostgreSQL\n- Study Amazon EC2 Auto Scaling 09/19/2025 09/19/2025 AWS Study Group Week 2 Achievements Completed Amazon S3 labs and learned bucket creation, upload/download, and access management. Completed Amazon RDS labs, created databases, connected, and managed data. Self-studied and understood the basics of Spring Boot (project structure, dependencies, basic API creation). Learned about Amazon CloudWatch and practiced labs for monitoring, metrics, and alarms. Learned PostgreSQL and understood basic integration with Spring Boot. Completed Amazon Lightsail and Lightsail Container labs, deployed sample apps, and managed containers. Understood Amazon EC2 Auto Scaling mechanism and how instances scale automatically based on demand. "
},
{
	"uri": "//localhost:1313/1-introduce/week3/",
	"title": "WEEK 3",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 3 Objectives Set up the base project for backend development. Learn and practice Amazon Route 53. Continue learning about Amazon EC2 Auto Scaling. Develop core authentication features: register, login, and JWT services. Study and practice with Amazon CLI. Collaborate with the team to review project progress. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Set up base project for development\n- Study about Amazon Route 53\n- Do: Amazon EC2 Auto Scaling labs 09/22/2025 09/22/2025 AWS Study Group 3 - Start developing register feature\n- Do: Amazon Route 53 labs 09/23/2025 09/23/2025 AWS Study Group 4 - Continue developing register feature\n- Study about Amazon CLI 09/24/2025 09/24/2025 AWS Study Group 5 - Team meeting about project progress\n- Complete register and start developing login feature and JWT services 09/25/2025 09/25/2025 AWS Study Group 6 - Finish login and JWT services\n- Do: Amazon CLI labs 09/26/2025 09/26/2025 AWS Study Group Week 3 Achievements Successfully set up the base project structure for development. Completed labs on Amazon EC2 Auto Scaling and Amazon Route 53, gaining knowledge in scaling and DNS management. Implemented user registration feature and later expanded with login and JWT-based authentication. Practiced using Amazon CLI and completed related labs. Held a team meeting to discuss project progress and align development tasks. Completed core authentication flow (register + login + JWT services), providing a foundation for secure backend development. "
},
{
	"uri": "//localhost:1313/1-introduce/week4/",
	"title": "WEEK 4",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 4 Objectives Fix and optimize existing authentication features (register/login). Learn and integrate MoMo payment gateway into the project. Participate in team meetings to review progress and discuss project proposal. Study and practice with Amazon DynamoDB. Learn how to use Cloudinary for image uploads in Spring Boot. Complete labs on Amazon DynamoDB and integrate Cloudinary into the project. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Fix bugs in register/login services\n- Study about MoMo services and how to integrate in Spring Boot 09/29/2025 09/29/2025 AWS Study Group 3 - Team meeting about project progress\n- Develop and integrate MoMo payment in project 09/30/2025 09/30/2025 4 - Team meeting to discuss project proposal and assign different tasks for members\n- Finish MoMo payment integration 10/01/2025 10/01/2025 5 - Study about DynamoDB\n- Learn about Cloudinary and how to implement it for image uploads in Spring Boot 10/02/2025 10/02/2025 AWS Study Group 6 - Do labs about DynamoDB\n- Integrate Cloudinary in the project 10/03/2025 10/03/2025 AWS Study Group Week 4 Achievements Fixed and improved register/login services for better stability. Successfully learned and integrated MoMo payment gateway into the backend project. Participated in team meetings to review progress and coordinate new development tasks. Gained hands-on experience with Amazon DynamoDB through labs and practice. Learned how to use Cloudinary for efficient image storage and integrated it into the project. Strengthened overall backend functionality with payment and media upload features, preparing the system for future expansion. "
},
{
	"uri": "//localhost:1313/1-introduce/week5/",
	"title": "WEEK 5",
	"tags": [],
	"description": "",
	"content": "WORKLOG Week 5 Objectives Learn and practice Amazon ElastiCache to improve application performance through caching. Study and implement Google OAuth 2 for multi-login authentication options. Research and understand Amazon CloudFront and its use for content delivery and website acceleration. Complete labs for ElastiCache and CloudFront to gain practical experience. Study Edge Computing concepts with Amazon CloudFront and Lambda@Edge. Research and integrate MoMo IPN (Instant Payment Notification) to verify payment status. Learn and practice integrating Amazon S3 for file storage, upload, and download management. Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Study about Amazon ElastiCache\n- Learn about Google OAuth 2 to integrate multiple login options in the project 10/06/2025 10/06/2025 AWS Study Group 3 - Study and research about Amazon CloudFront\n- Develop and integrate OAuth 2 into the project 10/07/2025 10/07/2025 AWS Study Group 4 - Do labs about Amazon ElastiCache and Amazon CloudFront\n- Finish integrating OAuth 2 10/08/2025 10/08/2025 AWS Study Group 5 - Study about Edge Computing with Amazon CloudFront and Lambda@Edge\n- Research about MoMo IPN to verify paid status 10/09/2025 10/09/2025 AWS Study Group 6 - Integrate MoMo IPN into project\n- Learn about integrating Amazon S3 to store, upload, and download files 10/10/2025 10/10/2025 AWS Study Group Week 5 Achievements Gained understanding of Amazon ElastiCache and its benefits for caching and performance optimization. Successfully implemented Google OAuth 2 for multiple login options, enhancing authentication flexibility. Completed labs and research on Amazon CloudFront, learning about CDN setup and content distribution. Practiced Edge Computing using Lambda@Edge to handle requests closer to users for lower latency. Integrated MoMo IPN into the project to automatically verify and confirm successful transactions. Learned and applied Amazon S3 integration for secure file storage, upload, and download functionality. Improved project scalability, authentication, and reliability through practical use of multiple AWS services. "
},
{
	"uri": "//localhost:1313/1-introduce/week6/",
	"title": "WEEK 6",
	"tags": [],
	"description": "",
	"content": "WEEK 6 WORKLOG Week 6 Objectives Integrate Amazon S3 into the project to support image upload and download functionality. Complete AWS CloudFront and Lambda@Edge labs to strengthen content delivery knowledge. Study Windows Workloads on AWS and Directory Services using AWS Managed Microsoft AD. Research and evaluate third-party APIs for calculating shipping fees, particularly Giao Hang Tiet Kiem (GHTK). Begin integrating the GHTK API into the existing project. Reinforce understanding of Secure Architectures in preparation for the midterm exam. Explore AWS VM Import/Export for virtual machine migration concepts. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Integrating S3 to the project with upload and download functions for images - Do labs related to Amazon CloudFront and Lambda@Edge 10/13/2025 10/13/2025 AWS Study Group 3 - Study about Windows Workloads on AWS and Directory Services with AWS Managed Microsoft AD - Considering and evaluating services to use to calculate shipping fee in the project 10/14/2025 10/14/2025 AWS Study Group 4 - Finish labs related to Windows Workloads on AWS and Directory Services with AWS Managed Microsoft AD - Research and study about Giao Hang Tiet Kiem API to calculate shipping fee 10/15/2025 10/15/2025 AWS Study Group GHTK API Docs 5 - Start to integrate Giao Hang Tiet Kiem API into the project - Do labs about Building Highly Available Web Applications - Join meeting about topic of Reinventing DevSecOps 10/16/2025 10/16/2025 AWS Study Group 6 - Revise knowledge about Secure Architecture to prepare for midterm exam - Continue to integrate Giao Hang Tiet Kiem API - Read about topic VM Migration with AWS VM Import/Export 10/17/2025 10/17/2025 AWS Study Group Week 6 Achievements Successfully integrated Amazon S3 into the project, enabling image upload and download features. Completed practical labs on AWS CloudFront and Lambda@Edge, gaining insights into edge computing and CDN optimization. Studied and finished labs on Windows Workloads and AWS Managed Microsoft AD, understanding how to manage hybrid environments. Researched and tested the GHTK shipping fee API, beginning integration into the project using Spring Boot. Participated in a team meeting about “Reinventing DevSecOps,” discussing security integration in CI/CD pipelines. Reviewed Secure Architecture principles, including IAM, encryption, and WAF, for midterm preparation. Read and summarized key points about VM migration using AWS VM Import/Export tools. "
},
{
	"uri": "//localhost:1313/1-introduce/week7/",
	"title": "WEEK 7",
	"tags": [],
	"description": "",
	"content": "WEEK 7 WORKLOG Week 7 Objectives Strengthen understanding of AWS architecture topics, including Resilient and High-Performing Architectures, in preparation for the midterm exam. Complete VM Migration using AWS VM Import/Export labs. Continue integrating and debugging the GHTK API for shipping fee calculation. Learn and apply concepts of Database Migration using AWS DMS and SCT. Study AWS Cost Optimization and Operational Excellence principles. Explore AWS Cognito for authentication integration in the ongoing project. Finalize the application architecture diagram using AWS services. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Revise knowledge about Resilient Architecture and services like EC2, S3,… to prepare for midterm exam - Complete VM Migration with AWS VM Import/Export labs - Continue to integrate and fix bugs with GHTK API 10/20/2025 10/20/2025 AWS Study Group GHTK API Docs 3 - Fix minor bugs with shopping cart after paying - Revised knowledge about High-Performing Architectures and services like RDS, DynamoDB,… 10/21/2025 10/21/2025 AWS Study Group 4 - Finish integrating GHTK API to calculate shipping fee - Read Database Migration with AWS Database Migration Service (DMS) and Schema Conversion Tool (SCT) - Team meeting to discuss and fix application diagram using AWS services 10/22/2025 10/22/2025 AWS Study Group GHTK API Docs 5 - Revise knowledge about AWS Cost Optimization and related services - Study about AWS Cognito 10/23/2025 10/23/2025 AWS Study Group 6 - Learn about Operational Excellence in AWS - Continue to learn about AWS Cognito to integrate into the project - Finish up application diagram with AWS services 10/24/2025 10/24/2025 AWS Study Group Week 7 Achievements Successfully revised and reinforced knowledge of AWS EC2, S3, RDS, and DynamoDB for resilient and high-performing architectures. Completed hands-on VM Migration using AWS VM Import/Export. Fully integrated and tested the GHTK API for calculating shipping fees within the project. Resolved bugs related to the shopping cart after payment. Collaboratively refined and finalized the application diagram with AWS services during team discussions. Gained a solid understanding of AWS Cost Optimization and Operational Excellence. Studied AWS Cognito and began planning its integration for user authentication. "
},
{
	"uri": "//localhost:1313/1-introduce/week8/",
	"title": "WEEK 8",
	"tags": [],
	"description": "",
	"content": "WEEK 8 WORKLOG Week 8 Objectives Learn and integrate AWS Cognito into the ongoing project to implement secure user authentication. Review and strengthen understanding of AWS core services (EC2, S3, IAM, MFA, SCP, Encryption, Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager). Study Resilient Architectures concepts including Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore in preparation for the AWS mid-term exam. Complete sample practice exams to evaluate and improve AWS service knowledge. Task Carried Out This Week Day Task Start Date Completion Date Reference Material 2 - Learning AWS Cognito to integrate into the project - Redo labs related to EC2 and S3 services to strengthen knowledge for mid-term exam 10/27/2025 10/27/2025 AWS Study Group 3 - Start to integrate AWS Cognito into the project - Read articles and information related to IAM, MFA, SCP, and Encryption to prepare for the mid-term exam 10/28/2025 10/28/2025 AWS Study Group KungFuTech AWS Course 4 - Research about Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager to understand Secure Architectures - Do sample exams related to AWS services 10/29/2025 10/29/2025 AWS Study Group KungFuTech AWS Course Practice Exam 5 - Read articles related to Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore to understand Resilient Architectures - Do sample exams related to AWS services to strengthen knowledge 10/30/2025 10/30/2025 AWS Study Group KungFuTech AWS Course Practice Exam 6 - Participate in AWS mid-term exam 10/31/2025 10/31/2025 Week 8 Achievements Successfully learned how to use AWS Cognito and began integrating it into the project for authentication management. Revisited and completed multiple labs on EC2 and S3, reinforcing hands-on practical skills. Conducted extensive research on AWS security and resilience best practices, improving understanding of secure and scalable architectures. Completed several AWS practice exams, identifying areas for improvement and building exam confidence. Actively participated in the AWS mid-term exam, applying knowledge to practical and test-based scenarios. "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " Week 1: Getting familiar with AWS and basic AWS services. Week 2: Learning core AWS services (S3, RDS, CloudWatch, Lightsail, EC2 Auto Scaling) and practicing backend setup with Spring Boot and PostgreSQL. Week 3: Setting up the backend project, implementing authentication (register, login, JWT), and practicing with Route 53, EC2 Auto Scaling, and AWS CLI. Week 4: Improving backend authentication and integrating MoMo payment and Cloudinary while practicing core AWS services. Week 5: Exploring advanced AWS services and integrating OAuth 2, MoMo IPN, and Amazon S3 into the project. Week 6: Focusing on integrating Amazon S3 and the GHTK API, completing AWS labs, studying secure architectures, and preparing for the midterm exam. Week 7: Deepening understanding of AWS architectures and services while exploring AWS Cognito for project enhancement. Week 8: Doing task H… Week 9: Doing task I… Week 10: Doing task L… Week 11: Doing task M… Week 12: Doing task N… "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createiamrole/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Create IAM Role In this step, we will proceed to create IAM Role. In this IAM Role, the policy AmazonSSMManagedInstanceCore will be assigned, this is the policy that allows the EC2 server to communicate with the Session Manager.\nGo to IAM service administration interface In the left navigation bar, click Roles. Click Create role. Click AWS service and click EC2. Click Next: Permissions. In the Search box, enter AmazonSSMManagedInstanceCore and press Enter to search for this policy. Click the policy AmazonSSMManagedInstanceCore. Click Next: Tags. Click Next: Review. Name the Role SSM-Role in Role Name Click Create Role . Next, we will make the connection to the EC2 servers we created with Session Manager.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.2-createpublicsubnet/",
	"title": "Create Public Subnet",
	"tags": [],
	"description": "",
	"content": "Create Public Subnet Click Subnets. Click Create subnet. At the Create subnet page. In the VPC ID section, click Lab VPC. In the Subnet name field, enter Lab Public Subnet. In the Availability Zone section, select the first Availability zone. In the field IPv4 CIRD block enter 10.10.1.0/24. Scroll to the bottom of the page, click Create subnet.\nClick Lab Public Subnet.\nClick Actions. Click Edit subnet settings. Click Enable auto-assign public IPv4 address. Click Save. Click Internet Gateways. Click Create internet gateway. At the Create internet gateway page. In the Name tag field, enter Lab IGW. Click Create internet gateway. After successful creation, click Actions. Click Attach to VPC. At the Attach to VPC page. In the Available VPCs section, select Lab VPC. Click Attach internet gateway. Check the successful attaching process as shown below. Next we will create a custom route table to assign to Lab Public Subnet. Click Route Tables. Click Create route table. At the Create route table page. In the Name field, enter Lab Publicrtb. In the VPC section, select Lab VPC. Click Create route table. After creating the route table successfully. Click Edit routes. At the Edit routes page. Click Add route. In the Destination field, enter 0.0.0.0/0 In the Target section, select Internet Gateway and then select Lab IGW. Click Save changes. Click the Subnet associations tab. Click Edit subnet associations to proceed with the associate custom route table we just created in Lab Public Subnet. At the Edit subnet associations page. Click on Lab Public Subnet. Click Save associations. Check that the route table information has been associated with Lab Public Subnet and the internet route information has been pointed to the Internet Gateway as shown below. "
},
{
	"uri": "//localhost:1313/4-s3log/4.2-creates3bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will create an S3 bucket to store session logs sent from EC2 instances.\nCreate S3 Bucket Access S3 service management console Click Create bucket. At the Create bucket page. In the Bucket name field, enter the bucket name lab-yourname-bucket-0001 In the Region section, select Region you are doing the current lab. The name of the S3 bucket must not be the same as all other S3 buckets in the system. You can substitute your name and enter a random number when generating the S3 bucket name.\nScroll down and click Create bucket. When we created the S3 bucket we did Block all public access so our EC2 instances won\u0026rsquo;t be able to connect to S3 via the internet. In the next step, we will configure the S3 Gateway Endpoint feature to allow EC2 instances to connect to the S3 bucket via the VPC\u0026rsquo;s internal network.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Proposal ",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.3-createprivatesubnet/",
	"title": "Create Private Subnet",
	"tags": [],
	"description": "",
	"content": "Create Private Subnet Click Subnets. Click Create subnet. At the Create subnet page. In the VPC ID section, click Lab VPC. In the Subnet name field, enter Lab Private Subnet. In the Availability Zone section, select the first Availability zone. In the field IPv4 CIRD block enter 10.10.2.0/24. Scroll to the bottom of the page, click Create subnet. The next step is to create the necessary security groups for the lab.\n"
},
{
	"uri": "//localhost:1313/4-s3log/4.3-creategwes3/",
	"title": "Create S3 Gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Go to VPC service management console Click Endpoints. Click Create endpoint. At the Create endpoint page. In the Name tag field, enter S3GW. In the Service Category section, click AWS services. In the search box enter S3, then select com.amazonaws.[region].s3 In the Services section, select com.amazonaws.[region].s3 with the Type of Gateway. In the VPC section, select Lab VPC. In the Route tables section, select both route tables. Scroll down, click Create endpoint. The next step is to configure Session Manager to store session logs to the S3 bucket we created.\n"
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Translated blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1- Accelerate Marketing Campaign Planning by 3x with Treasure Data AI Agents Powered by Amazon Bedrock The article explains how Treasure Data, in partnership with Amazon Bedrock (AWS), uses AI Agents to accelerate marketing campaign planning by 3x. These agents—like Audience Agent, Research Agent, and Migration Agent—analyze customer data, segment audiences, and automate insights without technical expertise. With AI Agent Foundry, businesses can create custom agents to meet specific needs while maintaining security and compliance. A case study with Nobitel showed campaign planning time cut by threefold and a 20% performance boost. Overall, the solution empowers marketers to focus on strategy while AI handles analysis, driving faster, data-driven, and more personalized marketing.\nBlog 2- Build resilient generative AI agents The article by Yiwen Zhang, Hechmi Khelifi, and Jennifer Moran outlines how to build resilient generative AI agents for production. It presents a seven-part framework covering models, infrastructure, tools, and security, and identifies five failure modes like latency and inaccurate responses. The authors suggest solutions such as fault isolation, redundancy, and human-in-the-loop validation, combining DevOps with AI-specific safeguards like Amazon Bedrock Guardrails. Overall, it offers guidance to ensure reliable, scalable AI deployments.\nBlog 3- Seamlessly burst EDA jobs to AWS using Synopsys Cloud Hybrid solution The article highlights how Synopsys Cloud Hybrid, powered by AWS, lets semiconductor teams easily run EDA workloads on the cloud without manual data transfers. It syncs on-prem and cloud data in real time using Amazon EC2 and FSx for NetApp ONTAP, delivering equal or better performance than on-prem setups. Supporting tools like VCS and PrimeTime, it also cuts setup time by weeks, offering a scalable, fast, and cost-efficient solution for chip design on AWS.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.4-createsecgroup/",
	"title": "Create security groups",
	"tags": [],
	"description": "",
	"content": "Create security groups In this step, we will proceed to create the security groups used for our instances. As you can see, these security groups will not need to open traditional ports to ssh like port 22 or remote desktop through port 3389.\nCreate security group for Linux instance located in public subnet Go to VPC service management console Click Security Group. Click Create security group. In the Security group name field, enter SG Public Linux Instance. In the Description section, enter SG Public Linux Instance. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Keep Outbound rule, drag the mouse to the bottom. Click Create security group. As you can see, the security group we created to use for Linux public instances will not need to open traditional ports to ssh like port 22.\nCreate a security group for a Windows instance located in a private subnet After successfully creating a security group for the Linux instance located in the public subnet, click the Security Groups link to return to the Security groups list. Click Create security group.\nIn the Security group name field, enter SG Private Windows Instance.\nIn the Description section, enter SG Private Windows Instance. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Scroll down. Add Outbound rule to allow TCP 443 connection to 10.10.0.0/16 ( CIDR of Lab VPC we created) Click Create security group. For the Instance in the private subnet, we will connect to the Session Manager endpoint over a TLS encrypted connection, so we need to allow outbound connection from our instance to VPC CIDR through port 443.\nCreate security group for VPC Endpoint In this step, we will create security group for VPC Endpoint of Session Manager. After successfully creating the security group for the Windows instance in the private subnet, click the Security Groups link to return to the Security groups list. Click Create security group. In the Security group name field, enter SG VPC Endpoint. In the Description section, enter SG VPC Endpoint. In the VPC section, click the X to reselect the Lab VPC you created for this lab. Scroll down. Delete Outbound rule. Add Inbound rule allowing TCP 443 to come from 10.10.0.0/16 ( CIDR of Lab VPC we created ). Click Create security group. So we are done creating the necessary security groups for EC2 instances and VPC Endpoints.\n"
},
{
	"uri": "//localhost:1313/4-s3log/",
	"title": "Events participated",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/4-s3log/4.4-configsessionlogs/",
	"title": "Monitor session logs",
	"tags": [],
	"description": "",
	"content": "Monitor session logs Access System Manager - Session Manager service management console Click the Preferences tab. Click Edit. Scroll down, at S3 logging, click Enable. Uncheck Allow only encrypted S3 buckets. Click Choose a bucket name from the list. Select the S3 bucket you created. Scroll down, click Save to save the configuration.\nAccess System Manager - Session Manager service management console\nClick Start session. Click Private Windows Instance. Click Start session. Type the command ipconfig. Type the command hostname. Click Terminate to exit the session, click Terminate again to confirm. Check Session logs in S3 Go to S3 service management console Click on the name of the S3 bucket we created for the lab. Click on the object name sessions log On the objects detail page, click Open. Object logs will be opened in a new tab in the browser. You can view the stored commands in session logs. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.5-createec2linux/",
	"title": "Create Public instance",
	"tags": [],
	"description": "",
	"content": " Go to EC2 service management console Click Instances. Click Launch instances. On the Step 1: Choose an Amazon Machine Image (AMI) page. Click Select to select AMI Amazon Linux 2 AMI. On the Step 2: Choose an Instance Type page. Click on Instance type t2.micro. Click Next: Configure Instance Details. At Step 3: Configure Instance Details page In the Network section, select Lab VPC. In the Subnet section, select Lab Public Subnet. In the Auto-assign Public IP section, select Use subnet setting (Enable) Click Next: Add Storage. Click Next: Add Tags to move to the next step. Click Next: Configure Security Group to move to the next step. On page Step 6: Configure Security Group. Select Select an existing security group. Select security group SG Public Linux Instance. Click Review and Launch. The warning dialog box appears because we do not configure the firewall to allow connections to port 22, Click Continue to continue.\nAt page Step 7: Review Instance Launch.\nClick Launch. In the Select an existing key pair or create a new key pair dialog box. Click to select Create a new key pair. In the Key pair name field, enter LabKeypair. Click Download Key Pair and save it to your computer. Click Launch Instances to create EC2 server. Click View Instances to return to the list of EC2 instances.\nClick the edit icon under the Name column.\nIn the Edit Name dialog box, enter Public Linux Instance. Click Save. Next, we will do the same to create an EC2 Instance Windows running in the Private subnet.\n"
},
{
	"uri": "//localhost:1313/5-portfwd/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nWe will configure Port Forwarding for the RDP connection between our machine and Private Windows Instance located in the private subnet we created for this exercise.\nCreate IAM user with permission to connect SSM Go to IAM service management console Click Users , then click Add users. At the Add user page. In the User name field, enter Portfwd. Click on Access key - Programmatic access. Click Next: Permissions. Click Attach existing policies directly.\nIn the search box, enter ssm. Click on AmazonSSMFullAccess. Click Next: Tags, click Next: Reviews. Click Create user. Save Access key ID and Secret access key information to perform AWS CLI configuration.\nInstall and Configure AWS CLI and Session Manager Plugin To perform this hands-on, make sure your workstation has AWS CLI and Session Manager Plugin installed -manager-working-with-install-plugin.html)\nMore hands-on tutorials on installing and configuring the AWS CLI can be found here.\nWith Windows, when extracting the Session Manager Plugin installation folder, run the install.bat file with Administrator permission to perform the installation.\nImplement Portforwarding Run the command below in Command Prompt on your machine to configure Port Forwarding. aws ssm start-session --target (your ID windows instance) --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region (your region) Windows Private Instance Instance ID information can be found when you view the EC2 Windows Private Instance server details.\nExample command: C:\\Windows\\system32\u0026gt;aws ssm start-session --target i-06343d7377486760c --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region ap-southeast-1 If your command gives an error like below: SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found\nProve that you have not successfully installed the Session Manager Plugin. You may need to relaunch Command Prompt after installing Session Manager Plugin.\nConnect to the Private Windows Instance you created using the Remote Desktop tool on your workstation. In the Computer section: enter localhost:9999. Return to the administration interface of the System Manager - Session Manager service. Click tab Session history. We will see session logs with Document name AWS-StartPortForwardingSession. Congratulations on completing the lab on how to use Session Manager to connect and store session logs in S3 bucket. Remember to perform resource cleanup to avoid unintended costs.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.6-createec2windows/",
	"title": "Create Private Instance",
	"tags": [],
	"description": "",
	"content": " Go to EC2 service management console Click Instances. Click Launch instances. On the Step 1: Choose an Amazon Machine Image (AMI) page. Drag the mouse down. Click Select to select AMI Microsoft Windows Server 2019 Base. On the Step 2: Choose an Instance Type page. Click on Instance type t2.micro. Click Next: Configure Instance Details. At Step 3: Configure Instance Details page In the Network section, select Lab VPC. In the Subnet section, select Lab Private Subnet. At Auto-assign Public IP select Use subnet setting (Disable) Click Next: Add Storage. Click Next: Add Tags to move to the next step. Click Next: Configure Security Group to move to the next step. On page Step 6: Configure Security Group. Select Select an existing security group. Select security group SG Private Windows Instance. Click Review and Launch. The warning dialog box appears because we do not configure the firewall to allow connections to port 22, Click Continue to continue.\nAt page Step 7: Review Instance Launch.\nClick Launch. In the Select an existing key pair or create a new key pair dialog box. Click Choose an existing key pair. In the Key pair name section, select LabKeypair. Click I acknowledge that I have access to the corresponding private key file, and that without this file, I won\u0026rsquo;t be able to log into my instance.. Click Launch Instances to create EC2 server. Click View Instances to return to the list of EC2 instances.\nClick the edit icon under the Name column.\nIn the Edit Name dialog box, enter Private Windows Instance. Click Save. Next, we will proceed to create IAM Roles to serve the Session Manager.\n"
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Self-assessment",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "//localhost:1313/6-cleanup-copy/",
	"title": "Sharing and feedback",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]